import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
import numpy as np
import random
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
from tqdm import tqdm
import argparse

# Set random seeds for reproducibility
RANDOM_SEED = 42

def set_seed(seed=RANDOM_SEED):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Model configurations
MODEL_CONFIGS = {
    "llama3.2-1B": "/home/junjie_chen/models/llama3.2-1B",
    "llama3-8B": "/home/junjie_chen/models/llama3-8B",
    # "llama-7B": "/home/junjie_chen/models/llama-7B",  # æš‚æ—¶ç§»é™¤ï¼štorchç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
    "qwen3-8B": "/home/junjie_chen/models/qwen3-8B",
}

def gen_kv_states(model, tokenizer, sentence, layers=[1], device_map="cuda"):
    """
    ç”ŸæˆæŒ‡å®šå±‚çš„Kå’ŒV states
    
    Returns:
        tuple: (k_states_list, v_states_list)
    """
    token_ids = tokenizer.encode(sentence, return_tensors="pt", add_special_tokens=False).to(device_map)
    
    with torch.no_grad():
        outputs = model(token_ids, use_cache=True, output_hidden_states=True)
    
    k_states_list = []
    v_states_list = []
    
    for layer_idx in layers:
        k_cache = outputs.past_key_values[layer_idx][0]
        v_cache = outputs.past_key_values[layer_idx][1]
        
        batch_size, num_heads, seq_len, head_dim = k_cache.shape
        k_states = k_cache.squeeze(0).transpose(0, 1).reshape(seq_len, num_heads * head_dim)
        v_states = v_cache.squeeze(0).transpose(0, 1).reshape(seq_len, num_heads * head_dim)
        
        k_states_list.append(k_states)
        v_states_list.append(v_states)
    
    return k_states_list, v_states_list

def gen_next_proposal(model, token_ids):
    """ä½¿ç”¨æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡æ’åº"""
    with torch.no_grad():
        output = model(token_ids)
    logits = output.logits[0, -1]
    return torch.argsort(logits, descending=True).long()

def permute_states(states: torch.Tensor, perm_type: str) -> torch.Tensor:
    """å¯¹statesè¿›è¡Œç½®æ¢"""
    N, d = states.size()
    device = states.device
    if perm_type == "None":
        return states
    elif perm_type == "S":
        return states[torch.randperm(N, device=device)]
    elif perm_type == "D":
        return states[:, torch.randperm(d, device=device)]
    elif perm_type == "SD":
        return permute_states(permute_states(states, "D"), "S")
    else:
        raise Exception(f"Unsupported permutation pattern {perm_type}")

def kv_matching_attack(
    model,
    tokenizer,
    perm_k_states: torch.Tensor,
    perm_v_states: torch.Tensor,
    layer: int,
    batch_sz: int = 128,
    matching_eps: float = 1.0,
    next_token_proposal: bool = True,
    max_proposal_candidates: int = 5000,
    device_map: str = "cuda",
    ground_truth_tokens: list[int] = None,
) -> list[int]:
    """
    ä½¿ç”¨KV cacheæ‰§è¡Œvocabulary matching attack
    
    Args:
        model: æ¨¡å‹
        tokenizer: tokenizer
        perm_k_states: Permuted K states (num_tokens, k_dim)
        perm_v_states: Permuted V states (num_tokens, v_dim)
        layer: è¦æ”»å‡»çš„å±‚
        batch_sz: æ‰¹æ¬¡å¤§å°
        matching_eps: åŒ¹é…é˜ˆå€¼
        next_token_proposal: æ˜¯å¦ä½¿ç”¨next token proposal
        max_proposal_candidates: æœ€å¤§å€™é€‰æ•°
        device_map: è®¾å¤‡
    
    Returns:
        è§£ç å‡ºçš„tokenåˆ—è¡¨
    """
    vocab_sz = model.config.vocab_size
    num_tokens = perm_k_states.shape[0]
    
    input_tokens = []
    
    for i in range(num_tokens):
        global_best_error = float('inf')
        global_best_token = None
        
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨next token proposalå†³å®šæœç´¢ç­–ç•¥
        if not next_token_proposal or i == 0:
            token_ids = torch.arange(0, vocab_sz, device=device_map).long()
            max_search_tokens = vocab_sz
        else:
            token_ids = gen_next_proposal(
                model,
                torch.LongTensor(input_tokens).unsqueeze(0).to(device_map)
            )
            max_search_tokens = min(max_proposal_candidates, vocab_sz) if max_proposal_candidates > 0 else vocab_sz
        
        # æ‰¹é‡å¤„ç†tokens
        for batch_start in range(0, max_search_tokens, batch_sz):
            batch_end = min(batch_start + batch_sz, max_search_tokens)
            actual_batch_sz = batch_end - batch_start
            
            # æ„å»ºbatchè¾“å…¥
            batch_ids = token_ids[batch_start:batch_end].reshape(-1, 1)
            
            if i > 0:
                batch_input_tokens = (
                    torch.tensor(input_tokens)
                    .to(device_map)
                    .reshape(1, -1)
                    .repeat(actual_batch_sz, 1)
                )
                batch_ids = torch.cat([batch_input_tokens, batch_ids], dim=-1).long()
            
            # Forward passè·å–KV cache
            with torch.no_grad():
                outputs = model(batch_ids, use_cache=True, output_hidden_states=True)
            
            # æå–Kå’ŒV
            k_cache = outputs.past_key_values[layer][0]  # (batch, num_heads, seq_len, head_dim)
            v_cache = outputs.past_key_values[layer][1]
            
            # åªå–æœ€åä¸€ä¸ªtoken
            batch_size, num_heads, seq_len, head_dim = k_cache.shape
            batch_k = k_cache[:, :, -1, :].reshape(batch_size, num_heads * head_dim)
            batch_v = v_cache[:, :, -1, :].reshape(batch_size, num_heads * head_dim)
            
            # è®¡ç®—Kå’ŒVçš„L1è·ç¦»ï¼ˆä½¿ç”¨æ’åºï¼‰
            perm_k_row = perm_k_states[i, :]
            perm_v_row = perm_v_states[i, :]
            
            sorted_perm_k, _ = torch.sort(perm_k_row)
            sorted_perm_v, _ = torch.sort(perm_v_row)
            
            # è®¡ç®—æ¯ä¸ªå€™é€‰çš„Kå’ŒV error
            batch_best_error = float('inf')
            batch_best_token = None
            
            for j in range(actual_batch_sz):
                sorted_k, _ = torch.sort(batch_k[j])
                sorted_v, _ = torch.sort(batch_v[j])
                
                k_error = torch.sum(torch.abs(sorted_perm_k - sorted_k)).item()
                v_error = torch.sum(torch.abs(sorted_perm_v - sorted_v)).item()
                total_error = k_error + v_error
                
                if total_error < global_best_error:
                    global_best_error = total_error
                    global_best_token = token_ids[batch_start + j].item()
                
                if total_error < batch_best_error:
                    batch_best_error = total_error
                    batch_best_token = token_ids[batch_start + j].item()
            
            # æ¸…ç†GPUå†…å­˜
            del outputs
            torch.cuda.empty_cache()
            
            # å¦‚æœæ‰¾åˆ°ä½äºmatching_epsçš„tokenï¼Œç«‹å³åœæ­¢ï¼ˆä¸åŸç‰ˆé€»è¾‘ä¸€è‡´ï¼‰
            if batch_best_error < matching_eps:
                global_best_error = batch_best_error
                global_best_token = batch_best_token
                break
            
            # å¦‚æœè¿™æ˜¯æœ€åä¸€ä¸ªbatchä¸”è¿˜æ²¡æ‰¾åˆ°ä½äºepsçš„ï¼Œæ‰“å°è­¦å‘Š
            if batch_end >= max_search_tokens and global_best_error > matching_eps:
                print(f"âš  No match for token {i} under eps={matching_eps:.4f}")
                print(f"   Best error: {global_best_error:.4f} for token {global_best_token} ('{tokenizer.decode([global_best_token])}')")
        
        # è®°å½•æ‰¾åˆ°çš„æœ€ä½³token
        input_tokens.append(global_best_token)
        
        # æ‰“å°è¿›åº¦
        status = "âœ“" if global_best_error < matching_eps else "âš "
        print(f"{status} Token {i}: {global_best_token} ('{tokenizer.decode([global_best_token])}'), "
              f"error={global_best_error:.4f}, eps={matching_eps:.4f}")
    
    return input_tokens

def run_kv_attack(
    model,
    tokenizer,
    sentence: str,
    layer: int,
    perm_type: str = "D",
    batch_sz: int = 128,
    matching_eps: float = 1.0,
    next_token_proposal: bool = True,
    max_proposal_candidates: int = 5000,
    device_map: str = "cuda",
) -> tuple[list[int], list[int]]:
    """
    æ‰§è¡Œå®Œæ•´çš„KV cacheæ”»å‡»æµç¨‹
    
    Returns:
        (ground_truth_tokens, decoded_tokens)
    """
    # è·å–ground truth
    ground_truth_tokens = tokenizer.encode(sentence, add_special_tokens=False)
    
    # ç”ŸæˆKV states
    print(f"ğŸ”‘ Attacking KV Cache at layer {layer}")
    k_states_list, v_states_list = gen_kv_states(model, tokenizer, sentence, layers=[layer], device_map=device_map)
    k_states = k_states_list[0]
    v_states = v_states_list[0]
    
    # åº”ç”¨ç½®æ¢
    perm_k_states = permute_states(k_states, perm_type)
    perm_v_states = permute_states(v_states, perm_type)
    
    # æ‰§è¡Œæ”»å‡»
    decoded_tokens = kv_matching_attack(
        model,
        tokenizer,
        perm_k_states,
        perm_v_states,
        layer,
        batch_sz=batch_sz,
        matching_eps=matching_eps,
        next_token_proposal=next_token_proposal,
        max_proposal_candidates=max_proposal_candidates,
        device_map=device_map,
    )
    
    return ground_truth_tokens, decoded_tokens

def truncate_prompt(tokenizer, text: str, num_tokens: int) -> str:
    """æˆªæ–­promptåˆ°æŒ‡å®šçš„tokenæ•°é‡"""
    token_ids = tokenizer.encode(text, add_special_tokens=False)[:num_tokens]
    return tokenizer.decode(token_ids, skip_special_tokens=True)

def load_lmsys_samples(tokenizer, num_samples=100, max_tokens=50, seed=RANDOM_SEED):
    """ä»lmsysæ•°æ®é›†åŠ è½½æ ·æœ¬"""
    data_file = "/home/junjie_chen/datasets/lmsys-chat-1m-data-1000/data.json"
    print(f"Loading samples from {data_file} (seed={seed})...")
    
    with open(data_file, 'r') as f:
        all_data = json.load(f)
    
    all_valid_samples = []
    for text in all_data:
        text = text.strip()
        if len(text) > 20:
            truncated = truncate_prompt(tokenizer, text, max_tokens)
            if len(truncated) > 10:
                all_valid_samples.append(truncated)
    
    print(f"Found {len(all_valid_samples)} valid samples")
    
    if len(all_valid_samples) > num_samples:
        random.seed(seed)
        samples = random.sample(all_valid_samples, num_samples)
        print(f"Randomly sampled {num_samples} samples (seed={seed})")
    else:
        samples = all_valid_samples[:num_samples]
        print(f"Using first {len(samples)} samples")
    
    return samples

def load_config(config_file: str, model_name: str, layer: int) -> float:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½matching_eps"""
    try:
        with open(config_file, 'r') as f:
            config = json.load(f)
        
        if model_name in config:
            layer_str = str(layer)
            if layer_str in config[model_name]['layers']:
                return config[model_name]['layers'][layer_str]['matching_eps']
        
        print(f"âš ï¸  Warning: No config found for {model_name} layer {layer}, using default eps=1.0")
        return 1.0
    
    except FileNotFoundError:
        print(f"âš ï¸  Warning: Config file {config_file} not found, using default eps=1.0")
        return 1.0

def main():
    parser = argparse.ArgumentParser(description="KV Cache æ”»å‡»")
    parser.add_argument("--model", type=str, required=True,
                        choices=list(MODEL_CONFIGS.keys()),
                        help="è¦æ”»å‡»çš„æ¨¡å‹")
    parser.add_argument("--layers", nargs="+", type=int, default=None,
                        help="è¦æ”»å‡»çš„å±‚åˆ—è¡¨ï¼ˆé»˜è®¤ï¼šç¬¬ä¸€å±‚ã€ä¸­é—´å±‚ã€æœ€åä¸€å±‚ï¼‰")
    parser.add_argument("--perm_type", type=str, default="None",
                        choices=["None", "D"],
                        help="ç½®æ¢ç±»å‹: None=æ— ç½®æ¢, D=ç»´åº¦ç½®æ¢")
    parser.add_argument("--config", type=str, default=None,
                        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤æ ¹æ®perm_typeè‡ªåŠ¨é€‰æ‹©ï¼‰")
    parser.add_argument("--num_samples", type=int, default=100,
                        help="æµ‹è¯•æ ·æœ¬æ•°")
    parser.add_argument("--max_tokens", type=int, default=50,
                        help="æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§tokenæ•°")
    parser.add_argument("--batch_size", type=int, default=128,
                        help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--max_proposal_candidates", type=int, default=5000,
                        help="æœ€å¤§å€™é€‰æ•°")
    parser.add_argument("--output", type=str, default=None,
                        help="è¾“å‡ºç»“æœæ–‡ä»¶")
    
    args = parser.parse_args()
    
    # æ ¹æ®perm_typeè‡ªåŠ¨é€‰æ‹©é…ç½®æ–‡ä»¶
    if args.config is None:
        if args.perm_type == "None":
            args.config = "kv_attack_config_no_perm.json"
        elif args.perm_type == "D":
            args.config = "kv_attack_config_with_perm.json"
        else:
            args.config = "kv_attack_config.json"
    
    # è®¾ç½®éšæœºç§å­
    set_seed(RANDOM_SEED)
    print(f"Random seed set to: {RANDOM_SEED}")
    
    # åŠ è½½æ¨¡å‹
    model_path = MODEL_CONFIGS[args.model]
    device_map = "cuda"
    model_dtype = torch.bfloat16
    
    print(f"\n{'='*80}")
    print(f"Loading model: {args.model}")
    print(f"Model path: {model_path}")
    print(f"{'='*80}\n")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path, torch_dtype=model_dtype, attn_implementation="eager"
    ).to(device_map)
    model.eval()
    
    num_hidden_layers = model.config.num_hidden_layers
    print(f"Model loaded. Number of layers: {num_hidden_layers}")
    
    # ç¡®å®šè¦æ”»å‡»çš„å±‚
    # æ³¨æ„ï¼šå±‚ç´¢å¼•ä»0å¼€å§‹ï¼Œæ‰€ä»¥æœ€åä¸€å±‚æ˜¯ num_hidden_layers - 1
    if args.layers is None:
        first_layer = 0  # ä½¿ç”¨ç¬¬0å±‚ï¼ˆç¬¬ä¸€å±‚ï¼‰
        middle_layer = num_hidden_layers // 2
        last_layer = num_hidden_layers - 1  # æœ€åä¸€å±‚çš„ç´¢å¼•
        attack_layers = [first_layer, middle_layer, last_layer]
    else:
        attack_layers = args.layers
    
    print(f"Attack layers: {attack_layers}")
    print(f"  - First layer: {attack_layers[0] if len(attack_layers) > 0 else 'N/A'} (layer 0)")
    print(f"  - Middle layer: {attack_layers[1] if len(attack_layers) > 1 else 'N/A'}")
    print(f"  - Last layer: {attack_layers[2] if len(attack_layers) > 2 else 'N/A'}")
    print(f"Permutation type: {args.perm_type}")
    print(f"Config file: {args.config}")
    
    # åŠ è½½æ ·æœ¬
    test_samples = load_lmsys_samples(tokenizer, num_samples=args.num_samples, 
                                      max_tokens=args.max_tokens, seed=RANDOM_SEED)
    
    print(f"\nLoaded {len(test_samples)} samples\n")
    
    # è®¾ç½®è¾“å‡ºæ–‡ä»¶
    if args.output is None:
        perm_suffix = "no_perm" if args.perm_type == "None" else "with_perm"
        args.output = f"kv_attack_results_{args.model}_{perm_suffix}.json"
    
    # è¿è¡Œæ”»å‡»
    results = {
        'model_name': args.model,
        'perm_type': args.perm_type,
        'num_samples': len(test_samples),
        'attack_layers': attack_layers,
        'layers': []
    }
    perm_type = args.perm_type
    next_token_proposal = True
    
    for layer in attack_layers:
        print(f"\n{'='*80}")
        print(f"Attacking Layer {layer}")
        print(f"{'='*80}\n")
        
        # ä»é…ç½®åŠ è½½matching_eps
        matching_eps = load_config(args.config, args.model, layer)
        print(f"Using matching_eps: {matching_eps:.6f}")
        
        layer_results = {
            'layer': layer,
            'matching_eps': matching_eps,
            'perm_type': perm_type,
            'samples': []
        }
        
        for idx, prompt in enumerate(tqdm(test_samples, desc=f"Layer {layer}")):
            try:
                ground_truth_tokens, decoded_tokens = run_kv_attack(
                    model,
                    tokenizer,
                    prompt,
                    layer,
                    perm_type=perm_type,
                    batch_sz=args.batch_size,
                    matching_eps=matching_eps,
                    next_token_proposal=next_token_proposal,
                    max_proposal_candidates=args.max_proposal_candidates,
                    device_map=device_map,
                )
                
                original_text = tokenizer.decode(ground_truth_tokens, skip_special_tokens=True)
                predicted_text = tokenizer.decode(decoded_tokens, skip_special_tokens=True)
                success = (original_text == predicted_text)
                
                sample_result = {
                    'sample_idx': idx,
                    'original_text': original_text,
                    'predicted_text': predicted_text,
                    'original_tokens': ground_truth_tokens,
                    'predicted_tokens': decoded_tokens,
                    'success': success,
                    'num_tokens': len(ground_truth_tokens)
                }
                
                layer_results['samples'].append(sample_result)
                
                status = "âœ“" if success else "âœ—"
                print(f"\n{status} Sample {idx}: {'SUCCESS' if success else 'FAILED'}")
                print(f"  Original : {original_text[:80]}{'...' if len(original_text) > 80 else ''}")
                print(f"  Predicted: {predicted_text[:80]}{'...' if len(predicted_text) > 80 else ''}")
            
            except Exception as e:
                print(f"\nâŒ Error processing sample {idx}: {e}")
                import traceback
                traceback.print_exc()
        
        # ç»Ÿè®¡æˆåŠŸç‡
        successful = sum(1 for s in layer_results['samples'] if s['success'])
        total = len(layer_results['samples'])
        success_rate = successful / total if total > 0 else 0
        
        layer_results['statistics'] = {
            'total_samples': total,
            'successful': successful,
            'success_rate': success_rate
        }
        
        results['layers'].append(layer_results)
        
        print(f"\n{'='*80}")
        print(f"Layer {layer} å®Œæˆ!")
        print(f"  Success Rate: {successful}/{total} ({success_rate:.2%})")
        print(f"{'='*80}\n")
    
    # ä¿å­˜ç»“æœ
    with open(args.output, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n{'='*80}")
    print(f"ğŸ‰ æ”»å‡»å®Œæˆ! æœ€ç»ˆç»“æœ")
    print(f"{'='*80}")
    for layer_result in results['layers']:
        layer = layer_result['layer']
        stats = layer_result['statistics']
        print(f"Layer {layer}: {stats['successful']}/{stats['total_samples']} ({stats['success_rate']:.2%})")
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
    print(f"{'='*80}\n")

if __name__ == "__main__":
    main()

