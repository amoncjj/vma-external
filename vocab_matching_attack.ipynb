{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:42.583527Z",
     "iopub.status.busy": "2025-01-24T01:42:42.583194Z",
     "iopub.status.idle": "2025-01-24T01:42:47.444019Z",
     "shell.execute_reply": "2025-01-24T01:42:47.443410Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Gemma2Config, LlamaConfig\n",
    "from transformers.models.gemma2.modeling_gemma2 import apply_rotary_pos_emb, repeat_kv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:47.446506Z",
     "iopub.status.busy": "2025-01-24T01:42:47.446187Z",
     "iopub.status.idle": "2025-01-24T01:42:47.448596Z",
     "shell.execute_reply": "2025-01-24T01:42:47.448243Z"
    }
   },
   "outputs": [],
   "source": [
    "device_map = \"cuda\"\n",
    "# device_map = \"cpu\" # Uncomment if no gpu is available\n",
    "model_dtype = torch.bfloat16\n",
    "model_name_or_path = \"google/gemma-2-2b-it\"\n",
    "# model_name_or_path = \"meta-llama/Llama-3.1-8B-Instruct\" # Uncomment to use llama instead of gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:47.450202Z",
     "iopub.status.busy": "2025-01-24T01:42:47.450049Z",
     "iopub.status.idle": "2025-01-24T01:42:49.593763Z",
     "shell.execute_reply": "2025-01-24T01:42:49.593175Z"
    }
   },
   "outputs": [],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "MODEL = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=model_dtype, attn_implementation=\"eager\"\n",
    ").to(device_map)\n",
    "MODEL.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Hidden States and Next-Token Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:49.609492Z",
     "iopub.status.busy": "2025-01-24T01:42:49.609352Z",
     "iopub.status.idle": "2025-01-24T01:42:49.612023Z",
     "shell.execute_reply": "2025-01-24T01:42:49.611647Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_hidden_states(sentence, layers=[1]):\n",
    "    \"\"\"\n",
    "    Helper function to compute and return the hidden states from the specified layers\n",
    "    of the loaded model for a given input sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence to be encoded and passed through the model.\n",
    "        layers (list of int): A list of layer indices from which to extract hidden states.\n",
    "\n",
    "    Returns:\n",
    "        hidden_states (list of torch.Tensor): The hidden states from the specified layers.\n",
    "    \"\"\"\n",
    "    token_ids = TOKENIZER.encode(sentence, return_tensors=\"pt\").to(device_map)\n",
    "    output = MODEL.forward(token_ids, output_hidden_states=True)\n",
    "    hidden_states = [output.hidden_states[l] for l in layers]\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:49.613515Z",
     "iopub.status.busy": "2025-01-24T01:42:49.613369Z",
     "iopub.status.idle": "2025-01-24T01:42:49.615738Z",
     "shell.execute_reply": "2025-01-24T01:42:49.615376Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_next_proposal(token_ids):\n",
    "    \"\"\"\n",
    "    Proposes an optimized ordering of vocabulary tokens for use in the vocabulary\n",
    "    matching attack.\n",
    "\n",
    "    Instead of searching through the vocabulary in a fixed or random order, this\n",
    "    function returns token indices sorted by descending likelihood, as predicted\n",
    "    by the model given the input `token_ids`. This significantly improves the\n",
    "    efficiency of the vocabulary matching process.\n",
    "\n",
    "    Note:\n",
    "        The tokenizer used to generate `token_ids` and the tokenizer of the model\n",
    "        being attacked must be the same for correct alignment.\n",
    "\n",
    "    For further details, see Appendix A of our paper.\n",
    "\n",
    "    Args:\n",
    "        token_ids (torch.Tensor): Input token IDs (batch size 1) to condition the model.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Token indices sorted by descending likelihood.\n",
    "    \"\"\"\n",
    "    output = MODEL.forward(token_ids)\n",
    "    logits = output.logits[0, -1]\n",
    "    return torch.argsort(logits, descending=True).long()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass with Caching Across Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:49.617286Z",
     "iopub.status.busy": "2025-01-24T01:42:49.617150Z",
     "iopub.status.idle": "2025-01-24T01:42:49.623044Z",
     "shell.execute_reply": "2025-01-24T01:42:49.622663Z"
    }
   },
   "outputs": [],
   "source": [
    "def self_attn_cache(\n",
    "    config,\n",
    "    rot_emb,\n",
    "    q_proj,\n",
    "    k_proj,\n",
    "    v_proj,\n",
    "    o_proj,\n",
    "    hidden_states: torch.Tensor,\n",
    "    past_k_values: torch.Tensor,\n",
    "    past_v_values: torch.Tensor,\n",
    "    device_map=device_map,\n",
    "    model_dtype=torch.float32,\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Replacement for Attention.forward() when we wish to find the possible outputs\n",
    "    of a batch of tokens, given the previous tokens.\n",
    "\n",
    "    This implements caching which is *different* from standard KV-caching, as it caches\n",
    "    repeated computation across *batches*, not just within a batch. For example, given\n",
    "    inputs like:\n",
    "        [[2, 3, 4, 5],\n",
    "         [2, 3, 4, 7],\n",
    "         [2, 3, 4, 9]],\n",
    "    standard KV-caching would recompute attention for the prefix [2, 3, 4] three times.\n",
    "    This function avoids that by caching the shared prefix computation once.\n",
    "\n",
    "    Note that this implementation is specific to Gemma2Attention and LlamaAttention. This function will\n",
    "    need to be manually tailored if the attack was used on other models to match their\n",
    "    attention mechanism forward pass. If you do not wish to implement this function for\n",
    "    every model, set `use_cache` to False in `vocab_matching_attack`, at the expense of a slower attack.\n",
    "\n",
    "    Input:\n",
    "        - config: Model configuration.\n",
    "        - rot_emb: Rotary embedding function.\n",
    "        - q_proj, k_proj, v_proj, o_proj: Linear projection layers used in attention.\n",
    "        - hidden_states (torch.Tensor): Hidden states for the batch of possible (N+1)th tokens, shape (B, D_init).\n",
    "        - past_k_values (torch.Tensor): Key matrix from previous tokens, shape (H, N, D).\n",
    "        - past_v_values (torch.Tensor): Value matrix from previous tokens, shape (H, N, D).\n",
    "\n",
    "    Returns:\n",
    "        - O (torch.Tensor): Output of shape (B, D_init).\n",
    "        - K (torch.Tensor): Key vectors for the (N+1)st tokens, shape (B, H, D).\n",
    "        - V (torch.Tensor): Value vectors for the (N+1)st tokens, shape (B, H, D).\n",
    "    \"\"\"\n",
    "    B, _ = hidden_states.size()\n",
    "    num_heads, N, head_dim = past_k_values.size()\n",
    "\n",
    "    # Make sure data types of past values are correct.\n",
    "    if past_k_values.dtype != model_dtype:\n",
    "        past_k_values = past_k_values.to(dtype=model_dtype)\n",
    "    if past_v_values.dtype != model_dtype:\n",
    "        past_v_values = past_v_values.to(dtype=model_dtype)\n",
    "\n",
    "    # Linear projection, rotary embedding, and KV head repetition. No speedups can be done here, as this is for the batch of possible (N+1)st tokens.\n",
    "    Q = q_proj(hidden_states).view(B, num_heads, 1, head_dim)  # (B, H, 1, D)\n",
    "    K = k_proj(hidden_states).view(\n",
    "        B, config.num_key_value_heads, 1, head_dim\n",
    "    )  # (B, H_KV, 1, D)\n",
    "    V = v_proj(hidden_states).view(\n",
    "        B, config.num_key_value_heads, 1, head_dim\n",
    "    )  # (B, H_KV, 1, D)\n",
    "    token_pos = (\n",
    "        torch.arrange(N, N + 1).unsqueeze(0).to(device_map)\n",
    "    )  # Don't cast this, this should always be int64.\n",
    "    cos, sin = rot_emb(\n",
    "        V, token_pos\n",
    "    )  # Note: we only need a position_id for the (N+1)st token here.\n",
    "    Q, K = apply_rotary_pos_emb(Q, K, cos, sin)\n",
    "    K = repeat_kv(K, num_heads // config.num_key_value_heads)  # (B, H, 1, D)\n",
    "    V = repeat_kv(V, num_heads // config.num_key_value_heads)  # (B, H, 1, D)\n",
    "\n",
    "    # We use broadcasted matmult when applicable to speedup attention score computation, and only compute the last row.\n",
    "    A_past = torch.matmul(\n",
    "        Q, past_k_values.transpose(1, 2)\n",
    "    )  # (B, H, 1, D) @ (H, D, N) = (B, H, 1, N)\n",
    "    A_self = (Q * K).sum(dim=-1, keepdim=True)  # (B, H, 1, 1)\n",
    "    A = torch.cat([A_past, A_self], dim=-1)  # (B, H, 1, N+1)\n",
    "\n",
    "    # Attention score processing. We ignore the attention mask since the new token depends on all tokens so far.\n",
    "    if isinstance(config, Gemma2Config):\n",
    "        A = A * (config.query_pre_attn_scalar ** -0.5)\n",
    "        if config.attn_logit_softcapping is not None:\n",
    "            A = (\n",
    "                torch.tanh(A / config.attn_logit_softcapping)\n",
    "                * config.attn_logit_softcapping\n",
    "            )\n",
    "    elif isinstance(config, LlamaConfig):\n",
    "        A = A * (head_dim ** -0.5)\n",
    "    else:\n",
    "        raise Exception(\"Caching for this model architecture is not yet supported. Please set use_cache to False.\")\n",
    "    \n",
    "    A = softmax(A, dim=-1, dtype=torch.float32).to(\n",
    "        Q.dtype\n",
    "    )  # Always upcast to float32 before back to original dtype, regardless of model_dtype.\n",
    "\n",
    "    # We use broadcasted matmult when applicable to speedup attention-value multiplication, and only compute the last row.\n",
    "    O_past = torch.matmul(\n",
    "        A[..., :-1], past_v_values\n",
    "    )  # (B, H, 1, N) @ (H, N, D) = (B, H, 1, D)\n",
    "    O_self = A[..., -1:] * V  # (B, H, 1, 1) @ (B, H, 1, D) = (B, H, 1, D)\n",
    "    O = O_past + O_self  # (B, H, 1, D)\n",
    "\n",
    "    # Final linear projection (remove heads).\n",
    "    O = O.transpose(1, 2).contiguous()  # (B, 1, H, D)\n",
    "    O = O.reshape(B, -1).contiguous()  # (B, 1, H * D)\n",
    "    O = o_proj(O)  # (B, 1, D_init)\n",
    "    return O, K[..., 0, :], V[..., 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:49.624517Z",
     "iopub.status.busy": "2025-01-24T01:42:49.624374Z",
     "iopub.status.idle": "2025-01-24T01:42:49.629150Z",
     "shell.execute_reply": "2025-01-24T01:42:49.628765Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass_cache(\n",
    "    token_ids: torch.Tensor,\n",
    "    all_past_k_values: list[torch.Tensor],\n",
    "    all_past_v_values: list[torch.Tensor],\n",
    "    num_layers: int,\n",
    "    device_map=device_map,\n",
    "    model_dtype: torch.dtype = model_dtype,\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Replacement for AutoModelForCausalLM.forward() when we wish to find the possible outputs\n",
    "    of a batch of tokens, given the previous tokens.\n",
    "\n",
    "    This implements caching which is *different* from standard KV-caching, as it caches\n",
    "    repeated computation *across batches*, not just within a batch.\n",
    "\n",
    "    Note that, like `self_attn_cache`, this function needs to be tailored to the attacked model.\n",
    "    Otherwise, set `use_cache` to False in `vocab_matching_attack`.\n",
    "\n",
    "    Input:\n",
    "        - all_past_k_values (list[torch.Tensor]): List of length `num_layers`, each of shape (H, N, D),\n",
    "          containing key matrices from previous tokens.\n",
    "        - all_past_v_values (list[torch.Tensor]): List of length `num_layers`, each of shape (H, N, D),\n",
    "          containing value matrices from previous tokens.\n",
    "        - token_ids (torch.Tensor): Tensor of token IDs for the (N+1)st token, of shape (B,).\n",
    "        - num_layers (int): Number of decoder layers to forward through. If equal to total number of layers,\n",
    "          final normalization is applied.\n",
    "\n",
    "    Returns:\n",
    "        - hidden_states (torch.Tensor): Final hidden state outputs at specified layer, of shape (B, D_init).\n",
    "        - all_K (torch.Tensor): Cached key matrices at each layer, shape (num_layers, B, H, D).\n",
    "        - all_V (torch.Tensor): Cached value matrices at each layer, shape (num_layers, B, H, D).\n",
    "    \"\"\"\n",
    "    config = MODEL.config\n",
    "    model = MODEL.model\n",
    "    assert num_layers <= config.num_hidden_layers\n",
    "\n",
    "    # Embedding and pre-scaling.\n",
    "    hidden_states = model.embed_tokens(token_ids)\n",
    "    if isinstance(config, Gemma2Config):\n",
    "        hidden_states = hidden_states * torch.tensor(\n",
    "            config.hidden_size**0.5, dtype=hidden_states.dtype\n",
    "        )\n",
    "\n",
    "    # These will store cached key and value matrices of sizes (B, H, D) for each layer. B is the length of token_ids.\n",
    "    all_K, all_V = [], []\n",
    "\n",
    "    # Decoder blocks with caching.\n",
    "    for layer_idx in range(num_layers):\n",
    "        layer = model.layers[layer_idx]\n",
    "\n",
    "        # Extract public weights needed for cached attention.\n",
    "        q_proj = layer.self_attn.q_proj\n",
    "        k_proj = layer.self_attn.k_proj\n",
    "        v_proj = layer.self_attn.v_proj\n",
    "        o_proj = layer.self_attn.o_proj\n",
    "        rot_emb = layer.self_attn.rotary_emb\n",
    "\n",
    "        # Past key and value matrices of shape (num_heads, num_tokens, head_dim) that the adversary may have deciphered thus far.\n",
    "        past_k_values = all_past_k_values[layer_idx]\n",
    "        past_v_values = all_past_v_values[layer_idx]\n",
    "\n",
    "        # Attention block. We remember to cache the K and V to return later.\n",
    "        residual = hidden_states\n",
    "        hidden_states = layer.input_layernorm(hidden_states)\n",
    "        hidden_states, K, V = self_attn_cache(\n",
    "            config,\n",
    "            rot_emb,\n",
    "            q_proj,\n",
    "            k_proj,\n",
    "            v_proj,\n",
    "            o_proj,\n",
    "            hidden_states,\n",
    "            past_k_values,\n",
    "            past_v_values,\n",
    "            device_map=device_map,\n",
    "            model_dtype=model_dtype,\n",
    "        )\n",
    "        if isinstance(config, Gemma2Config):\n",
    "            hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        all_K.append(K)\n",
    "        all_V.append(V)\n",
    "\n",
    "        # MLP block.\n",
    "        residual = hidden_states\n",
    "        if isinstance(config, Gemma2Config):\n",
    "            hidden_states = layer.pre_feedforward_layernorm(hidden_states)\n",
    "            hidden_states = layer.mlp(hidden_states)    \n",
    "            hidden_states = layer.post_feedforward_layernorm(hidden_states)\n",
    "        elif isinstance(config, LlamaConfig):\n",
    "            hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "            hidden_states = layer.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "    # Final normalization for the last layer.\n",
    "    if num_layers >= config.num_hidden_layers:\n",
    "        hidden_states = model.norm(hidden_states)\n",
    "\n",
    "    all_K, all_V = torch.stack(all_K), torch.stack(all_V)\n",
    "    return hidden_states, all_K, all_V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab Matching Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:49.630638Z",
     "iopub.status.busy": "2025-01-24T01:42:49.630500Z",
     "iopub.status.idle": "2025-01-24T01:42:49.633667Z",
     "shell.execute_reply": "2025-01-24T01:42:49.633305Z"
    }
   },
   "outputs": [],
   "source": [
    "def permute_states(hidden_states: torch.Tensor, perm_type: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies a permutation to the hidden states tensor based on the specified permutation type.\n",
    "\n",
    "    Possible permutation types are:\n",
    "        * 'None' -- No permutation applied.\n",
    "        * 'S'    -- Sequence-level permutation (shuffle rows).\n",
    "        * 'D'    -- Per-token hidden dimension permutation (shuffle columns).\n",
    "        * 'SD'   -- Combined sequence and hidden dimension permutation (apply 'D' then 'S').\n",
    "\n",
    "    Args:\n",
    "        hidden_states (torch.Tensor): Input tensor of shape (N, d), where N is the number of tokens\n",
    "                                      and d is the hidden dimension.\n",
    "        perm_type (str): Type of permutation to apply.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Permuted tensor of shape (N, d).\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the permutation type is not supported.\n",
    "    \"\"\"\n",
    "    N, d = hidden_states.size()\n",
    "    device = hidden_states.device\n",
    "    if perm_type == \"None\":\n",
    "        return hidden_states\n",
    "    elif perm_type == \"S\":\n",
    "        return hidden_states[torch.randperm(N, device=device)]\n",
    "    elif perm_type == \"D\":\n",
    "        return hidden_states[:, torch.randperm(d, device=device)]\n",
    "    elif perm_type == \"SD\":\n",
    "        return permute_states(permute_states(hidden_states, \"D\"), \"S\")\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported permutation pattern {perm_type}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Matching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:49.635133Z",
     "iopub.status.busy": "2025-01-24T01:42:49.634995Z",
     "iopub.status.idle": "2025-01-24T01:42:49.637959Z",
     "shell.execute_reply": "2025-01-24T01:42:49.637596Z"
    }
   },
   "outputs": [],
   "source": [
    "def argmin_except(tensor: torch.Tensor, ignore_indices: list[int]) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Returns the 2D index of the minimum value in a tensor, excluding specified column indices.\n",
    "\n",
    "    This function finds the global minimum across all elements in a 2D tensor,\n",
    "    except for positions in columns listed in `ignore_indices`, which are masked out.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): A 2D tensor of shape (B, N) to search for the minimum value.\n",
    "        ignore_indices (list[int]): List of column indices to ignore in the search.\n",
    "\n",
    "    Returns:\n",
    "        tuple[int, int]: The (batch_index, sequence_index) corresponding to the minimum value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a mask of valid positions (False for ignored indices)\n",
    "    mask = torch.ones(tensor.shape[1], dtype=torch.bool, device=tensor.device)\n",
    "    mask[ignore_indices] = False\n",
    "\n",
    "    # Expand mask to match tensor dimensions\n",
    "    mask = mask.expand(tensor.shape)\n",
    "\n",
    "    # Set ignored positions to maximum value\n",
    "    masked_tensor = torch.where(mask, tensor, torch.finfo(tensor.dtype).max)\n",
    "\n",
    "    # Get global argmin across both dimensions\n",
    "    flat_idx = masked_tensor.view(-1).argmin()\n",
    "\n",
    "    # Convert flat index back to 2D indices\n",
    "    batch_idx = flat_idx // tensor.shape[1]\n",
    "    seq_idx = flat_idx % tensor.shape[1]\n",
    "\n",
    "    return batch_idx.item(), seq_idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:49.639402Z",
     "iopub.status.busy": "2025-01-24T01:42:49.639261Z",
     "iopub.status.idle": "2025-01-24T01:42:49.643295Z",
     "shell.execute_reply": "2025-01-24T01:42:49.642938Z"
    }
   },
   "outputs": [],
   "source": [
    "def l1_dist_matching(\n",
    "    perm_hidden_states: torch.Tensor,\n",
    "    vocab_hidden_states: torch.Tensor,\n",
    "    index: int,\n",
    "    ignore_perm_idx: list[int] = [],\n",
    ") -> tuple[tuple[int, int], float]:\n",
    "    \"\"\"\n",
    "    Matches by closest L1 distance between a row vector from `perm_hidden_states` and all row vectors in `vocab_hidden_states`.\n",
    "\n",
    "    This function works for `None` and `S` permutations in `perm_hidden_states`. You can specify indices to ignore from\n",
    "    matching using `ignore_perm_idx`.\n",
    "\n",
    "    Args:\n",
    "        perm_hidden_states (torch.Tensor): Tensor of permuted hidden states of shape (N, D).\n",
    "        vocab_hidden_states (torch.Tensor): Tensor of vocab hidden states of shape (V, 1, D).\n",
    "        index (int): Index of the row in perm_hidden_states to match.\n",
    "        ignore_perm_idx (list[int], optional): Indices in perm_hidden_states to ignore. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        tuple[tuple[int, int], float]: Tuple of best-matching indices (perm_index, vocab_index) and the L1 distance.\n",
    "    \"\"\"\n",
    "    perm_hidden_states = perm_hidden_states[index, :]\n",
    "    intermediate = torch.sum(\n",
    "        torch.abs(perm_hidden_states - vocab_hidden_states[:, -1:, :]), dim=-1\n",
    "    )\n",
    "    vocab_argmin = torch.argmin(intermediate)\n",
    "    min_l1_dist = intermediate[vocab_argmin].item()\n",
    "    return (index, vocab_argmin), min_l1_dist\n",
    "\n",
    "\n",
    "def l1_dist_matching_permuted(\n",
    "    perm_hidden_states: torch.Tensor,\n",
    "    vocab_hidden_states: torch.Tensor,\n",
    "    index: int | None = None,\n",
    "    ignore_perm_idx: list[int] = [],\n",
    ") -> tuple[tuple[int, int], float]:\n",
    "    \"\"\"\n",
    "    Matches by closest L1 distance between all pairs of row vectors between `perm_hidden_states` and `vocab_hidden_states`.\n",
    "\n",
    "    This function handles arbitrary permutations and returns the best match among all combinations,\n",
    "    excluding any permuted indices listed in `ignore_perm_idx`.\n",
    "\n",
    "    Args:\n",
    "        perm_hidden_states (torch.Tensor): Tensor of permuted hidden states of shape (N, D).\n",
    "        vocab_hidden_states (torch.Tensor): Tensor of vocab hidden states of shape (V, 1, D).\n",
    "        index (int | None, optional): Unused placeholder for compatibility. Defaults to None.\n",
    "        ignore_perm_idx (list[int], optional): Indices in perm_hidden_states to ignore. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        tuple[tuple[int, int], float]: Tuple of best-matching indices (perm_index, vocab_index) and the L1 distance.\n",
    "    \"\"\"\n",
    "    intermediate = torch.sum(\n",
    "        torch.abs(perm_hidden_states.unsqueeze(0) - vocab_hidden_states[:, -1:, :]),\n",
    "        dim=-1,\n",
    "    )\n",
    "    vocab_argmin, seq_argmin = argmin_except(\n",
    "        intermediate, ignore_indices=ignore_perm_idx\n",
    "    )\n",
    "    min_l1_dist = intermediate[vocab_argmin, seq_argmin].item()\n",
    "    return (seq_argmin, vocab_argmin), min_l1_dist\n",
    "\n",
    "\n",
    "def l1_sort_dist_matching(\n",
    "    perm_hidden_states: torch.Tensor,\n",
    "    vocab_hidden_states: torch.Tensor,\n",
    "    index: int | None = None,\n",
    "    ignore_perm_idx: list[int] = [],\n",
    ") -> tuple[tuple[int, int], float]:\n",
    "    \"\"\"\n",
    "    Matches by closest L1 distance between sorted row vectors of 2D tensors.\n",
    "\n",
    "    This function supports all permutation types (`None`, `S`, `D`, `SD`) by sorting the vectors before matching.\n",
    "    It calls `l1_dist_matching` on the sorted tensors.\n",
    "\n",
    "    Args:\n",
    "        perm_hidden_states (torch.Tensor): Tensor of permuted hidden states of shape (N, D).\n",
    "        vocab_hidden_states (torch.Tensor): Tensor of vocab hidden states of shape (V, 1, D).\n",
    "        index (int | None, optional): Row index to match. Defaults to None.\n",
    "        ignore_perm_idx (list[int], optional): Indices in perm_hidden_states to ignore. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        tuple[tuple[int, int], float]: Tuple of best-matching indices (perm_index, vocab_index) and the L1 distance.\n",
    "    \"\"\"\n",
    "    sorted_phs, _ = torch.sort(perm_hidden_states, dim=-1)\n",
    "    sorted_vhs, _ = torch.sort(vocab_hidden_states, dim=-1)\n",
    "    return l1_dist_matching(\n",
    "        sorted_phs, sorted_vhs, index, ignore_perm_idx=ignore_perm_idx\n",
    "    )\n",
    "\n",
    "\n",
    "def l1_sort_dist_matching_permuted(\n",
    "    perm_hidden_states: torch.Tensor,\n",
    "    vocab_hidden_states: torch.Tensor,\n",
    "    index: int | None = None,\n",
    "    ignore_perm_idx: list[int] = [],\n",
    ") -> tuple[tuple[int, int], float]:\n",
    "    \"\"\"\n",
    "    Matches by closest L1 distance between all pairs of sorted row vectors.\n",
    "\n",
    "    This handles all permutation types by sorting the vectors in both tensors first, then matching\n",
    "    all pairs using `l1_dist_matching_permuted`.\n",
    "\n",
    "    Args:\n",
    "        perm_hidden_states (torch.Tensor): Tensor of permuted hidden states of shape (N, D).\n",
    "        vocab_hidden_states (torch.Tensor): Tensor of vocab hidden states of shape (V, 1, D).\n",
    "        index (int | None, optional): Unused placeholder for compatibility. Defaults to None.\n",
    "        ignore_perm_idx (list[int], optional): Indices in perm_hidden_states to ignore. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        tuple[tuple[int, int], float]: Tuple of best-matching indices (perm_index, vocab_index) and the L1 distance.\n",
    "    \"\"\"\n",
    "    sorted_phs, _ = torch.sort(perm_hidden_states, dim=-1)\n",
    "    sorted_vhs, _ = torch.sort(vocab_hidden_states, dim=-1)\n",
    "    return l1_dist_matching_permuted(\n",
    "        sorted_phs, sorted_vhs, ignore_perm_idx=ignore_perm_idx\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Vocab Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:49.644860Z",
     "iopub.status.busy": "2025-01-24T01:42:49.644709Z",
     "iopub.status.idle": "2025-01-24T01:42:49.653161Z",
     "shell.execute_reply": "2025-01-24T01:42:49.652704Z"
    }
   },
   "outputs": [],
   "source": [
    "def vocab_matching_attack(\n",
    "    perm_hidden_states: torch.Tensor,\n",
    "    num_layers: int,\n",
    "    matching_ftn: callable,\n",
    "    perm_type: str,\n",
    "    batch_sz: int = 1000,\n",
    "    matching_eps: float = 1e-3,\n",
    "    next_token_proposal: bool = False,\n",
    "    use_cache: bool = False,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Performs a full vocabulary matching attack to reconstruct tokens from hidden states using distance-based matching.\n",
    "\n",
    "    This generalized attack supports various permutation types and corresponding matching functions (`l1_dist_matching`,\n",
    "    `l1_dist_matching_permuted`, `l1_sort_dist_matching`, `l1_sort_dist_matching_permuted`). It optionally supports\n",
    "    next-token proposals using the model’s logits and can use key/value caching for efficient forward passes.\n",
    "\n",
    "    Args:\n",
    "        perm_hidden_states (torch.Tensor): Tensor of permuted hidden states of shape (N, D).\n",
    "        num_layers (int): Number of layers to extract hidden states from.\n",
    "        matching_ftn (callable): Matching function to use, must be compatible with the given permutation type.\n",
    "        perm_type (str): The permutation type, one of 'None', 'S', 'D', 'SD'.\n",
    "        batch_sz (int, optional): Batch size for forward passes. Defaults to 1000.\n",
    "        matching_eps (float, optional): Early stopping threshold for matching error. Defaults to 1e-3.\n",
    "        next_token_proposal (bool, optional): Whether to use LLM to suggest next token candidates. Defaults to False.\n",
    "        use_cache (bool, optional): Whether to use key/value caching to speed up matching. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of decoded token IDs corresponding to the best matches found.\n",
    "    \"\"\"\n",
    "\n",
    "    if use_cache:\n",
    "        MODEL.config.use_cache = True\n",
    "    else:\n",
    "        MODEL.config.use_cache = False\n",
    "\n",
    "    # Check matching attack/function is valid with permutation type.\n",
    "    if matching_ftn == l1_dist_matching:\n",
    "        assert perm_type in [\"None\"]\n",
    "    elif matching_ftn == l1_dist_matching_permuted:\n",
    "        assert perm_type in [\"None\", \"S\"]\n",
    "    elif matching_ftn == l1_sort_dist_matching:\n",
    "        assert perm_type in [\"None\", \"D\"]\n",
    "    elif matching_ftn == l1_sort_dist_matching_permuted:\n",
    "        assert perm_type in [\"None\", \"S\", \"D\", \"SD\"]\n",
    "    else:\n",
    "        raise Exception(\"Unsupported matching type currently\")\n",
    "\n",
    "    # Get embedding weights.\n",
    "    config = MODEL.config\n",
    "    vocab_sz = config.vocab_size\n",
    "\n",
    "    # Initialize cached key and value matrices. See `forward_pass_cache` for explanation of the shape.\n",
    "    if use_cache:\n",
    "        all_past_k_values = torch.empty(\n",
    "            num_layers, config.num_attention_heads, 0, config.head_dim\n",
    "        ).to(device_map)\n",
    "        all_past_v_values = torch.empty(\n",
    "            num_layers, config.num_attention_heads, 0, config.head_dim\n",
    "        ).to(device_map)\n",
    "\n",
    "    # Vocab attack deciphers token one at a time.\n",
    "    ignore_perm_idx = []\n",
    "    input_tokens = []\n",
    "    num_tokens = perm_hidden_states.numel() // config.hidden_size\n",
    "    for i in range(num_tokens):\n",
    "        global_best_error = 100000\n",
    "        global_best_token = None\n",
    "        # Based on our token proposal method, we either iterate through tokens in order of ID, or use the LLM to predict high-prob next tokens.\n",
    "        if not next_token_proposal or i == 0:\n",
    "            token_ids = torch.arrange(0, vocab_sz, device=device_map).long()\n",
    "        else:\n",
    "            token_ids = gen_next_proposal(\n",
    "                torch.LongTensor(input_tokens).unsqueeze(0).to(device_map)\n",
    "            )\n",
    "\n",
    "        # We batch tokens due to memory and time constraints.\n",
    "        for batch_start in range(0, vocab_sz, batch_sz):\n",
    "            # If caching, use `forward_pass_cache` to get the batched output for the next token.\n",
    "            if use_cache:\n",
    "                batch_ids = token_ids[\n",
    "                    batch_start : min(batch_start + batch_sz, vocab_sz)\n",
    "                ]\n",
    "                batch_hidden_states, all_K, all_V = forward_pass_cache(\n",
    "                    batch_ids, all_past_k_values, all_past_v_values, num_layers\n",
    "                )\n",
    "                batch_hidden_states = batch_hidden_states.reshape(batch_sz, 1, -1)\n",
    "\n",
    "            # Otherwise, directly run the model's forward pass on all batches for the next token, concatenating with previous known tokens.\n",
    "            else:\n",
    "                batch_ids = token_ids[\n",
    "                    batch_start : min(batch_start + batch_sz, vocab_sz)\n",
    "                ].reshape(-1, 1)\n",
    "                batch_input_tokens = (\n",
    "                    torch.tensor(input_tokens)\n",
    "                    .to(device_map)\n",
    "                    .reshape(1, -1)\n",
    "                    .repeat(batch_sz, 1)\n",
    "                )\n",
    "                batch_ids = torch.cat([batch_input_tokens, batch_ids], dim=-1).long()\n",
    "                outputs = MODEL.forward(batch_ids, output_hidden_states=True)\n",
    "                batch_hidden_states = outputs.hidden_states[num_layers]\n",
    "\n",
    "            best_pair, best_err = matching_ftn(\n",
    "                perm_hidden_states,\n",
    "                batch_hidden_states,\n",
    "                index=i,\n",
    "                ignore_perm_idx=ignore_perm_idx,\n",
    "            )\n",
    "\n",
    "            if global_best_error > best_err:\n",
    "                global_best_error = best_err\n",
    "                global_best_token = token_ids[batch_start + best_pair[1]].item()\n",
    "                global_ignored_idx = best_pair[0]\n",
    "\n",
    "                if use_cache:\n",
    "                    global_best_K = all_K[:, best_pair[1], :, None]\n",
    "                    global_best_V = all_V[:, best_pair[1], :, None]\n",
    "\n",
    "            if batch_start + batch_sz >= vocab_sz and global_best_error > matching_eps:\n",
    "                print(f\"No match for token {i} under eps\")\n",
    "                print(f\"Best error: {global_best_error} for token {global_best_token}\")\n",
    "\n",
    "            # If we get below the epsilon matching error, we choose this token and move on to the next token reversal.\n",
    "            # If the full vocabulary is exhausted, choose the token with lowest global error\n",
    "            if (i < num_tokens and best_err < matching_eps) or batch_start + batch_sz >= vocab_sz:\n",
    "                chosen_token = global_best_token\n",
    "                ignore_perm_idx.append(global_ignored_idx)\n",
    "\n",
    "                # Add the token to our list of known tokens before stopping batched run.\n",
    "                input_tokens.append(chosen_token)\n",
    "\n",
    "                # If caching, we must update the known key and value matrices.\n",
    "                if use_cache:\n",
    "                    all_past_k_values = torch.cat(\n",
    "                        [all_past_k_values, global_best_K], dim=-2\n",
    "                    )\n",
    "                    all_past_v_values = torch.cat(\n",
    "                        [all_past_v_values, global_best_V], dim=-2\n",
    "                    )\n",
    "                break\n",
    "\n",
    "    return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:49.654688Z",
     "iopub.status.busy": "2025-01-24T01:42:49.654544Z",
     "iopub.status.idle": "2025-01-24T01:42:49.658070Z",
     "shell.execute_reply": "2025-01-24T01:42:49.657591Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_vocab_matching_attack(\n",
    "    sentence: str,\n",
    "    layer: int,\n",
    "    dist_funct: str = \"l1\",\n",
    "    perm_type: str = \"None\",\n",
    "    batch_sz: int = 100,\n",
    "    matching_eps: float = 1e-3,\n",
    "    next_token_proposal: bool = False,\n",
    "    use_cache: bool = False,\n",
    ") -> tuple[list[int], list[int]]:\n",
    "    \"\"\"\n",
    "    Utility function that performs the full workflow of the vocabulary matching attack.\n",
    "\n",
    "    This includes tokenizing the input sentence, generating hidden states, applying a specified permutation type,\n",
    "    selecting a matching function, and invoking the main `vocab_matching_attack`. It optionally supports next-token\n",
    "    proposal and key/value caching to speed up matching.\n",
    "\n",
    "    Args:\n",
    "    sentence (str): Input prompt to be attacked.\n",
    "    layer (int): Transformer layer from which to extract hidden states.\n",
    "    dist_funct (str, optional): Matching function to use: one of 'l1', 'l1_permuted', 'l1_sort', or 'l1_sort_permuted'. Defaults to 'l1'.\n",
    "    perm_type (str, optional): Type of permutation to apply on the hidden states. One of 'None', 'S', 'D', or 'SD'. Defaults to 'None'.\n",
    "    batch_sz (int, optional): Number of candidate tokens evaluated per batch during matching. Defaults to 100.\n",
    "    matching_eps (float, optional): Threshold for early stopping based on matching error. Defaults to 1.0.\n",
    "    next_token_proposal (bool, optional): Whether to use model logits to prioritize candidate tokens. Defaults to False.\n",
    "    use_cache (bool, optional): Whether to use key/value caching to accelerate forward passes. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    tuple[list[int], list[int]]: A tuple containing the original token IDs and the decoded token IDs predicted by the attack.\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_states = gen_hidden_states(sentence, layers=[layer])[0][0]\n",
    "    perm_hidden_states = permute_states(hidden_states, perm_type)\n",
    "\n",
    "    if dist_funct == \"l1\":\n",
    "        dist_funct = l1_dist_matching\n",
    "    elif dist_funct == \"l1_permuted\":\n",
    "        dist_funct = l1_dist_matching_permuted\n",
    "    elif dist_funct == \"l1_sort\":\n",
    "        dist_funct = l1_sort_dist_matching\n",
    "    elif dist_funct == \"l1_sort_permuted\":\n",
    "        dist_funct = l1_sort_dist_matching_permuted\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dist_funct {dist_funct}\")\n",
    "\n",
    "    decoded = vocab_matching_attack(\n",
    "        perm_hidden_states,\n",
    "        layer,\n",
    "        dist_funct,\n",
    "        perm_type,\n",
    "        batch_sz=batch_sz,\n",
    "        matching_eps=matching_eps,\n",
    "        next_token_proposal=next_token_proposal,\n",
    "        use_cache=use_cache,\n",
    "    )\n",
    "    return decoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:49.659601Z",
     "iopub.status.busy": "2025-01-24T01:42:49.659457Z",
     "iopub.status.idle": "2025-01-24T01:42:49.661852Z",
     "shell.execute_reply": "2025-01-24T01:42:49.661438Z"
    }
   },
   "outputs": [],
   "source": [
    "def truncate_prompt(text: str, num_tokens: int) -> str:\n",
    "    \"\"\"\n",
    "    Helper function that encodes a prompt, truncates it to a specified number of tokens,\n",
    "    and decodes it back into text without adding special tokens.\n",
    "\n",
    "    This is useful for creating a prompt with a maximum token length constraint.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input prompt string.\n",
    "        num_tokens (int): The maximum number of tokens to retain.\n",
    "\n",
    "    Returns:\n",
    "        str: The truncated prompt string, decoded from the first `num_tokens` tokens.\n",
    "    \"\"\"\n",
    "    token_ids = TOKENIZER.encode(text, add_special_tokens=False)[:num_tokens]\n",
    "    return TOKENIZER.decode(token_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:49.605830Z",
     "iopub.status.busy": "2025-01-24T01:42:49.605682Z",
     "iopub.status.idle": "2025-01-24T01:42:49.607939Z",
     "shell.execute_reply": "2025-01-24T01:42:49.607552Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example prompts to test the vocabulary matching attack on.\n",
    "PROMPTS = [\n",
    "    \"Write a short story in which a robot discovers an ancient human diary. The robot interprets the diary as a set of instructions to rebuild humanity but encounters unexpected challenges. Describe its thought process and how it resolves its dilemmas.\",\n",
    "    \"If the sequence \\(a_n\\) is defined recursively by \\(a_0 = 2\\) and \\(a_{n+1} = 3a_n + 4\\), find a closed-form expression for \\(a_n\\). Explain your reasoning and solve for \\(a_5\\).\",\n",
    "    \"Explain the key factors that led to the fall of the Western Roman Empire. Focus on economic, military, and political causes, and discuss whether any single cause was decisive or if it was a combination of factors.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack parameters\n",
    "layer = 1\n",
    "dist_funct = \"l1_sort_permuted\"\n",
    "perm_type = \"SD\"\n",
    "batch_sz = 256\n",
    "matching_eps = 22\n",
    "next_token_proposal = True\n",
    "use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:42:49.663398Z",
     "iopub.status.busy": "2025-01-24T01:42:49.663257Z",
     "iopub.status.idle": "2025-01-24T15:12:33.660927Z",
     "shell.execute_reply": "2025-01-24T15:12:33.660340Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [truncate_prompt(prompt, 50) for prompt in PROMPTS]\n",
    "for prompt in prompts:\n",
    "    decoded_tokens = run_vocab_matching_attack(\n",
    "        prompt,\n",
    "        layer,\n",
    "        dist_funct,\n",
    "        perm_type,\n",
    "        batch_sz,\n",
    "        matching_eps,\n",
    "        next_token_proposal,\n",
    "        use_cache,\n",
    "    )\n",
    "    obtained_prediction = TOKENIZER.decode(decoded_tokens, skip_special_tokens=True)\n",
    "    print(obtained_prediction)\n",
    "    print(\"Results match!\" if prompt == obtained_prediction else \"Results don't match.\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "20fb44abfc064a6bb904a28c207078f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "229450ecbc2b441aa9a611b9a76f7456": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "26ecd53d12304fb194ec591a89cd4fb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2c0550f5b97648ed865c2d6cc80b4d47",
       "placeholder": "​",
       "style": "IPY_MODEL_d0a68e2f528940129a4d34b7db0daa24",
       "tabbable": null,
       "tooltip": null,
       "value": "Resolving data files: 100%"
      }
     },
     "2c0550f5b97648ed865c2d6cc80b4d47": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "312caf6397ec4d9eafc86f6d17189ea2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3402153366c243ba87678ef77d2661b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "37735b7c7cf840cdac2e2547bb25ea06": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "39774cb25c554f938b71170d49a56236": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5897cd508cde441a8053199f416e712a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "593eb123047c4a88b5a9db0aa4a5ea10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5982327c082e4f2ab8da2c41fee83137": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_20fb44abfc064a6bb904a28c207078f2",
       "placeholder": "​",
       "style": "IPY_MODEL_e4fa0eaf954e419d9cc913e7a727fcbe",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "5a9facb54aa740ee86918b827a3027e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_da8f60788d2c4da8b28befc2a848a500",
        "IPY_MODEL_e72483bfe7cc4285a601a44c6703ae12",
        "IPY_MODEL_80ed3dee8f3248c89262e9abd7d4e41e"
       ],
       "layout": "IPY_MODEL_89a94303847343e7ac5f723a27e0cb7f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5f323475f3f544299d0f71229204ba33": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5f766b333cad43539f4b4fe5384c6015": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_92917cafb45d4207bee35a8376a49edb",
       "placeholder": "​",
       "style": "IPY_MODEL_e49b9f3b2bc94cde89a24f02ef97f5ad",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [00:00&lt;00:00,  4.80it/s]"
      }
     },
     "700e1399ce5e4b138d3a405b60a078d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "78c4e528546840c6961f756974b58b96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_26ecd53d12304fb194ec591a89cd4fb3",
        "IPY_MODEL_81fe291f66c941e99579c1549d54052f",
        "IPY_MODEL_a167749e3fc740df8704de1b5e30e488"
       ],
       "layout": "IPY_MODEL_229450ecbc2b441aa9a611b9a76f7456",
       "tabbable": null,
       "tooltip": null
      }
     },
     "80ed3dee8f3248c89262e9abd7d4e41e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b33de3affe4749ea8ff3b56ee342666d",
       "placeholder": "​",
       "style": "IPY_MODEL_37735b7c7cf840cdac2e2547bb25ea06",
       "tabbable": null,
       "tooltip": null,
       "value": " 2080/2080 [00:00&lt;00:00,  9.06it/s]"
      }
     },
     "81fe291f66c941e99579c1549d54052f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d454194562c34e06991de79c893b051d",
       "max": 20,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_312caf6397ec4d9eafc86f6d17189ea2",
       "tabbable": null,
       "tooltip": null,
       "value": 20
      }
     },
     "89a94303847343e7ac5f723a27e0cb7f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8eb965ad89ca4ac297361138e9f463b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "92917cafb45d4207bee35a8376a49edb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a167749e3fc740df8704de1b5e30e488": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b4c7fb6fa0f54892a09b163a8ee2a0a8",
       "placeholder": "​",
       "style": "IPY_MODEL_5f323475f3f544299d0f71229204ba33",
       "tabbable": null,
       "tooltip": null,
       "value": " 20/20 [00:00&lt;00:00, 3920.64it/s]"
      }
     },
     "a33d5c662ac54ab8bd800c414fafe3fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5982327c082e4f2ab8da2c41fee83137",
        "IPY_MODEL_e59d50d3760c4d2ea487bc765a29332b",
        "IPY_MODEL_5f766b333cad43539f4b4fe5384c6015"
       ],
       "layout": "IPY_MODEL_3402153366c243ba87678ef77d2661b5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b33de3affe4749ea8ff3b56ee342666d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4c7fb6fa0f54892a09b163a8ee2a0a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d0a68e2f528940129a4d34b7db0daa24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d454194562c34e06991de79c893b051d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "da8f60788d2c4da8b28befc2a848a500": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8eb965ad89ca4ac297361138e9f463b9",
       "placeholder": "​",
       "style": "IPY_MODEL_593eb123047c4a88b5a9db0aa4a5ea10",
       "tabbable": null,
       "tooltip": null,
       "value": "Resolving data files: 100%"
      }
     },
     "e49b9f3b2bc94cde89a24f02ef97f5ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e4fa0eaf954e419d9cc913e7a727fcbe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e59d50d3760c4d2ea487bc765a29332b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fa90982183d84cb8b9e2be88b40edddb",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_700e1399ce5e4b138d3a405b60a078d3",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "e72483bfe7cc4285a601a44c6703ae12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_39774cb25c554f938b71170d49a56236",
       "max": 2080,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5897cd508cde441a8053199f416e712a",
       "tabbable": null,
       "tooltip": null,
       "value": 2080
      }
     },
     "fa90982183d84cb8b9e2be88b40edddb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
