#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¶é›†é«˜è´¨é‡å¥å­çš„è„šæœ¬ï¼š
1. å…ˆä»å·²å®Œæˆçš„æ”»å‡»ç»“æœä¸­æ”¶é›†åœ¨18ç§æƒ…å†µä¸‹åŒ¹é…ç‡éƒ½è¶…è¿‡95%çš„å¥å­
2. ç„¶åä»lmsys-chat-1m-dataæ•°æ®é›†ç»§ç»­é‡‡æ ·ï¼ŒæŒ¨ä¸ªå°è¯•æ”»å‡»
3. åªä¿ç•™è‹±æ–‡å¥å­ï¼Œé¿å…é‡å¤
4. ç›®æ ‡æ˜¯æ”¶é›†100ä¸ªå¥å­
5. æ¯æ”¶é›†ä¸€ä¸ªå¥å­å°±å®æ—¶å†™å…¥JSON
6. è¯¦ç»†çš„æ—¥å¿—è®°å½•
"""

import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
import numpy as np
import random
import json
import glob
import logging
from datetime import datetime
from tqdm import tqdm
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd
from typing import Optional

# éšæœºç§å­
RANDOM_SEED = 42

# è®¾ç½®æ—¥å¿—
def setup_logging(log_file="collect_sentences.log"):
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

def set_seed(seed=RANDOM_SEED):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Model configurations
MODEL_CONFIGS = {
    "llama3.2-1B": {
        "path": "/home/junjie_chen/models/llama3.2-1B",
        "layers": {"no_perm": [0, 8, 15], "with_perm": [0, 8, 15]}
    },
    "llama3-8B": {
        "path": "/home/junjie_chen/models/llama3-8B",
        "layers": {"no_perm": [0, 16, 31], "with_perm": [0, 16, 31]}
    },
    "qwen3-8B": {
        "path": "/home/junjie_chen/models/qwen3-8B",
        "layers": {"no_perm": [0, 18, 35], "with_perm": [0, 18, 35]}
    },
    "chatglm3-6B": {
        "path": "/home/junjie_chen/models/chatglm3-6B",
        "layers": {"no_perm": [0, 16, 31], "with_perm": [0, 16, 31]}
    },
    "llama3.2-3B": {
        "path": "/home/junjie_chen/models/llama3.2-3B",
        "layers": {"no_perm": [0, 8, 15], "with_perm": [0, 8, 15]}
    },
}

def is_english(text):
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸»è¦æ˜¯è‹±æ–‡"""
    if not text:
        return False
    ascii_chars = sum(1 for c in text if ord(c) < 128)
    ratio = ascii_chars / len(text)
    return ratio > 0.8

def load_existing_qualified_sentences(tokenizer, threshold=0.95):
    """ä»å·²æœ‰çš„_sentences.jsonæ–‡ä»¶ä¸­åŠ è½½æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„å¥å­"""
    sentence_files = glob.glob("*_sentences.json")
    all_sentences = {}  # {sentence_idx: {config_key: {"original": ..., "predicted": ...}}}
    
    logger.info(f"æ‰¾åˆ° {len(sentence_files)} ä¸ªå¥å­æ–‡ä»¶")
    
    for filepath in sentence_files:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        model_name = data.get('model_name', 'unknown')
        perm_type = data.get('perm_type', 'unknown')
        layer = data.get('layer', 'unknown')
        
        config_key = f"{model_name}_{perm_type}_layer{layer}"
        
        sentences = data.get('sentences', [])
        for sent in sentences:
            idx = sent.get('sample_idx', -1)
            if idx not in all_sentences:
                all_sentences[idx] = {}
            all_sentences[idx][config_key] = {
                'original': sent.get('original', ''),
                'predicted': sent.get('predicted', '')
            }
    
    # ç»Ÿè®¡æœ‰å¤šå°‘ç§é…ç½®
    all_configs = set()
    for idx, configs in all_sentences.items():
        all_configs.update(configs.keys())
    
    logger.info(f"æ‰¾åˆ° {len(all_configs)} ç§é…ç½®")
    
    qualified_sentences = []
    
    for idx, configs in sorted(all_sentences.items()):
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„é…ç½®æ•°æ®
        if len(configs) < len(all_configs):
            continue
        
        # è·å–åŸå§‹å¥å­
        original_sentence = list(configs.values())[0]['original']
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è‹±æ–‡
        if not is_english(original_sentence):
            continue
        
        # æ£€æŸ¥æ‰€æœ‰é…ç½®çš„åŒ¹é…ç‡
        all_qualified = True
        match_rates = {}
        results_for_sentence = {}
        
        for config_key, data in configs.items():
            original = data['original']
            predicted = data['predicted']
            
            orig_tokens = tokenizer.encode(original, add_special_tokens=False)
            pred_tokens = tokenizer.encode(predicted, add_special_tokens=False)
            
            if len(orig_tokens) == 0:
                match_rate = 0.0
            else:
                matches = sum(1 for o, p in zip(orig_tokens, pred_tokens) if o == p)
                match_rate = matches / len(orig_tokens)
            
            match_rates[config_key] = match_rate
            results_for_sentence[config_key] = {
                'original': original,
                'predicted': predicted,
                'match_rate': match_rate
            }
            
            if match_rate < threshold:
                all_qualified = False
        
        if all_qualified:
            qualified_sentences.append({
                'sentence': original_sentence,
                'sample_idx': idx,
                'source': 'existing_results',
                'match_rates': match_rates,
                'results': results_for_sentence
            })
    
    return qualified_sentences

def save_qualified_sentence(output_file, sentence_data):
    """ä¿å­˜å•ä¸ªç¬¦åˆæ¡ä»¶çš„å¥å­åˆ°JSONï¼ˆè‡ªåŠ¨å»é‡ï¼‰"""
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        data = {'sentences': []}
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå¥å­ï¼ˆé˜²æ­¢é‡å¤ï¼‰
    sentence_text = sentence_data.get('sentence', '')
    existing_sentences = [s.get('sentence', '') for s in data.get('sentences', [])]
    
    if sentence_text not in existing_sentences:
        data['sentences'].append(sentence_data)
        data['total_count'] = len(data['sentences'])
        data['last_updated'] = datetime.now().isoformat()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    else:
        logger.warning(f"å¥å­å·²å­˜åœ¨ï¼Œè·³è¿‡ä¿å­˜: {sentence_text[:50]}...")

def save_sentence_results(results_file, sentence_idx, sentence, results):
    """ä¿å­˜å•ä¸ªå¥å­åœ¨18ç§é…ç½®ä¸‹çš„è¯¦ç»†ç»“æœ"""
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        data = {'sentence_results': []}
    
    data['sentence_results'].append({
        'sentence_idx': sentence_idx,
        'original_sentence': sentence,
        'configs': results,
        'timestamp': datetime.now().isoformat()
    })
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def load_config(config_file: str, model_name: str, layer: int) -> float:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½matching_eps"""
    try:
        with open(config_file, 'r') as f:
            config = json.load(f)
        
        if model_name in config:
            layer_str = str(layer)
            if layer_str in config[model_name]['layers']:
                return config[model_name]['layers'][layer_str]['matching_eps']
        
        logger.warning(f"æœªæ‰¾åˆ° {model_name} layer {layer} çš„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼ 1.0")
        return 1.0
    except FileNotFoundError:
        logger.warning(f"é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼ 1.0")
        return 1.0

def gen_kv_states(model, tokenizer, sentence, layers=[1], device_map="cuda"):
    """ç”ŸæˆæŒ‡å®šå±‚çš„Kå’ŒV states"""
    token_ids = tokenizer.encode(sentence, return_tensors="pt", add_special_tokens=False).to(device_map)
    
    with torch.no_grad():
        outputs = model(token_ids, use_cache=True, output_hidden_states=True)
    
    k_states_list = []
    v_states_list = []
    
    for layer_idx in layers:
        k_cache = outputs.past_key_values[layer_idx][0]
        v_cache = outputs.past_key_values[layer_idx][1]
        
        batch_size, num_heads, seq_len, head_dim = k_cache.shape
        k_states = k_cache.squeeze(0).transpose(0, 1).reshape(seq_len, num_heads * head_dim)
        v_states = v_cache.squeeze(0).transpose(0, 1).reshape(seq_len, num_heads * head_dim)
        
        k_states_list.append(k_states)
        v_states_list.append(v_states)
    
    return k_states_list, v_states_list

def gen_next_proposal(model, token_ids):
    """ä½¿ç”¨æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡æ’åº"""
    with torch.no_grad():
        output = model(token_ids)
    logits = output.logits[0, -1]
    return torch.argsort(logits, descending=True).long()

def generate_permutation(N: int, d: int, perm_type: str, device: torch.device) -> tuple:
    """ç”Ÿæˆç½®æ¢ç´¢å¼•"""
    seq_perm = None
    dim_perm = None
    
    if perm_type == "S":
        seq_perm = torch.randperm(N, device=device)
    elif perm_type == "D":
        dim_perm = torch.randperm(d, device=device)
    elif perm_type == "SD":
        seq_perm = torch.randperm(N, device=device)
        dim_perm = torch.randperm(d, device=device)
    
    return seq_perm, dim_perm

def apply_permutation(states: torch.Tensor, seq_perm: torch.Tensor, dim_perm: torch.Tensor) -> torch.Tensor:
    """åº”ç”¨é¢„ç”Ÿæˆçš„ç½®æ¢"""
    result = states
    if dim_perm is not None:
        result = result[:, dim_perm]
    if seq_perm is not None:
        result = result[seq_perm]
    return result

def kv_matching_attack(
    model,
    tokenizer,
    perm_k_states: torch.Tensor,
    perm_v_states: torch.Tensor,
    layer: int,
    perm_type: str = "None",
    batch_sz: int = 128,
    matching_eps: float = 1.0,
    max_proposal_candidates: int = 5000,
    device_map: str = "cuda",
    ground_truth_tokens: list = None,
    verbose: bool = True,
    token_log_path: Optional[str] = None,
) -> tuple[list, bool]:
    """ä½¿ç”¨KV cacheæ‰§è¡Œvocabulary matching attack"""
    use_sort = (perm_type == "D" or perm_type == "SD")
    
    vocab_sz = model.config.vocab_size
    num_tokens = perm_k_states.shape[0]
    
    input_tokens = []
    aborted_due_to_eps = False
    token_log_f = None
    if token_log_path:
        os.makedirs(os.path.dirname(token_log_path), exist_ok=True)
        # JSONLï¼šæ¯ä¸ªtokenä¸€è¡Œï¼Œä¾¿äºå®æ—¶è¿½åŠ ä¸æ¢å¤æŸ¥çœ‹
        token_log_f = open(token_log_path, "a", encoding="utf-8")
    
    for i in range(num_tokens):
        # ç¬¬ä¸€ä¸ªtokenç›´æ¥ä½¿ç”¨æ­£ç¡®çš„token
        if i == 0 and ground_truth_tokens is not None and len(ground_truth_tokens) > 0:
            correct_token = ground_truth_tokens[0]
            input_tokens.append(correct_token)
            if verbose:
                print(f"        âœ“ Token {i}: {correct_token} ('{tokenizer.decode([correct_token])}') [ä½¿ç”¨æ­£ç¡®çš„token]")
            if token_log_f:
                token_log_f.write(json.dumps({
                    "token_index": i,
                    "token_id": int(correct_token),
                    "token_text": tokenizer.decode([correct_token]),
                    "best_error": 0.0,
                    "eps": float(matching_eps),
                    "status": "gt_first_token",
                }, ensure_ascii=False) + "\n")
                token_log_f.flush()
            continue
        
        global_best_error = float('inf')
        global_best_token = None
        
        token_ids = gen_next_proposal(
            model,
            torch.LongTensor(input_tokens).unsqueeze(0).to(device_map)
        )
        max_search_tokens = min(max_proposal_candidates, vocab_sz)
        
        for batch_start in range(0, max_search_tokens, batch_sz):
            batch_end = min(batch_start + batch_sz, max_search_tokens)
            actual_batch_sz = batch_end - batch_start
            
            batch_ids = token_ids[batch_start:batch_end].reshape(-1, 1)
            
            if i > 0:
                batch_input_tokens = (
                    torch.tensor(input_tokens)
                    .to(device_map)
                    .reshape(1, -1)
                    .repeat(actual_batch_sz, 1)
                )
                batch_ids = torch.cat([batch_input_tokens, batch_ids], dim=-1).long()
            
            with torch.no_grad():
                outputs = model(batch_ids, use_cache=True, output_hidden_states=True)
            
            k_cache = outputs.past_key_values[layer][0]
            v_cache = outputs.past_key_values[layer][1]
            
            batch_size, num_heads, seq_len, head_dim = k_cache.shape
            batch_k = k_cache[:, :, -1, :].reshape(batch_size, num_heads * head_dim)
            batch_v = v_cache[:, :, -1, :].reshape(batch_size, num_heads * head_dim)
            
            perm_k_row = perm_k_states[i, :]
            perm_v_row = perm_v_states[i, :]
            
            if use_sort:
                sorted_perm_k, _ = torch.sort(perm_k_row)
                sorted_perm_v, _ = torch.sort(perm_v_row)
            else:
                sorted_perm_k = perm_k_row
                sorted_perm_v = perm_v_row
            
            batch_best_error = float('inf')
            batch_best_token = None
            
            for j in range(actual_batch_sz):
                if use_sort:
                    sorted_k, _ = torch.sort(batch_k[j])
                    sorted_v, _ = torch.sort(batch_v[j])
                else:
                    sorted_k = batch_k[j]
                    sorted_v = batch_v[j]
                
                k_error = torch.sum(torch.abs(sorted_perm_k - sorted_k)).item()
                v_error = torch.sum(torch.abs(sorted_perm_v - sorted_v)).item()
                total_error = k_error + v_error
                
                if total_error < global_best_error:
                    global_best_error = total_error
                    global_best_token = token_ids[batch_start + j].item()
                
                if total_error < batch_best_error:
                    batch_best_error = total_error
                    batch_best_token = token_ids[batch_start + j].item()
            
            del outputs
            torch.cuda.empty_cache()
            
            if batch_best_error < matching_eps:
                global_best_error = batch_best_error
                global_best_token = batch_best_token
                break
            
            # å¦‚æœè¿™æ˜¯æœ€åä¸€ä¸ªbatchä¸”è¿˜æ²¡æ‰¾åˆ°ä½äºepsçš„ï¼Œæ‰“å°è­¦å‘Š
            if batch_end >= max_search_tokens and global_best_error > matching_eps:
                if verbose:
                    print(f"        âš  No match for token {i} under eps={matching_eps:.4f}")
                    print(f"           Best error: {global_best_error:.4f} for token {global_best_token} ('{tokenizer.decode([global_best_token])}')")
        
        input_tokens.append(global_best_token)
        
        # æ‰“å°è¿›åº¦
        if verbose:
            status = "âœ“" if global_best_error < matching_eps else "âš "
            print(f"        {status} Token {i}: {global_best_token} ('{tokenizer.decode([global_best_token])}'), "
                  f"error={global_best_error:.4f}, eps={matching_eps:.4f}")
        
        # å®æ—¶è®°å½•æ¯ä¸ªtokenï¼ˆJSONLï¼‰
        if token_log_f:
            token_log_f.write(json.dumps({
                "token_index": i,
                "token_id": int(global_best_token) if global_best_token is not None else None,
                "token_text": tokenizer.decode([global_best_token]) if global_best_token is not None else "",
                "best_error": float(global_best_error),
                "eps": float(matching_eps),
                "below_eps": bool(global_best_error < matching_eps),
            }, ensure_ascii=False) + "\n")
            token_log_f.flush()
        
        # å¦‚æœå½“å‰tokençš„æœ€ä¼˜è¯¯å·®ä»ç„¶è¶…è¿‡matching_epsï¼Œåˆ™æå‰ç»ˆæ­¢æœ¬å¥å­çš„æ”»å‡»ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¥å­
        if global_best_error > matching_eps:
            aborted_due_to_eps = True
            if verbose:
                print(f"        â­ï¸  æå‰ç»ˆæ­¢è¯¥å¥å­ï¼štoken {i} æœ€ä¼˜è¯¯å·® {global_best_error:.4f} > eps {matching_eps:.4f}")
            break
    
    if token_log_f:
        token_log_f.close()
    return input_tokens, aborted_due_to_eps

def run_single_attack(
    model,
    tokenizer,
    sentence,
    layer,
    perm_type,
    matching_eps,
    device_map="cuda",
    verbose=True,
    token_log_path: Optional[str] = None,
):
    """æ‰§è¡Œå•ä¸ªæ”»å‡»é…ç½®"""
    ground_truth_tokens = tokenizer.encode(sentence, add_special_tokens=False)
    
    if verbose:
        print(f"      ğŸ”‘ Attacking KV Cache at layer {layer}, perm_type={perm_type}")
        print(f"      ğŸ“ åŸæ–‡ ({len(ground_truth_tokens)} tokens): {sentence[:60]}{'...' if len(sentence) > 60 else ''}")
    
    k_states_list, v_states_list = gen_kv_states(model, tokenizer, sentence, layers=[layer], device_map=device_map)
    k_states = k_states_list[0]
    v_states = v_states_list[0]
    
    N, d = k_states.size()
    seq_perm, dim_perm = generate_permutation(N, d, perm_type, k_states.device)
    
    perm_k_states = apply_permutation(k_states, seq_perm, dim_perm)
    perm_v_states = apply_permutation(v_states, seq_perm, dim_perm)
    
    decoded_tokens, aborted_due_to_eps = kv_matching_attack(
        model,
        tokenizer,
        perm_k_states,
        perm_v_states,
        layer,
        perm_type=perm_type,
        batch_sz=128,
        matching_eps=matching_eps,
        max_proposal_candidates=5000,
        device_map=device_map,
        ground_truth_tokens=ground_truth_tokens,
        verbose=verbose,
        token_log_path=token_log_path,
    )
    
    if aborted_due_to_eps:
        if verbose:
            print(f"      â­ï¸  æœ¬é…ç½®æå‰ç»ˆæ­¢ï¼šè¯¯å·®è¶…è¿‡ matching_epsï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¥å­")
        return {
            'original': tokenizer.decode(ground_truth_tokens, skip_special_tokens=True),
            'predicted': "",
            'match_rate': 0.0,
            'success': False,
            'aborted_due_to_eps': True,
        }
    
    original_text = tokenizer.decode(ground_truth_tokens, skip_special_tokens=True)
    predicted_text = tokenizer.decode(decoded_tokens, skip_special_tokens=True)
    
    # è®¡ç®—åŒ¹é…ç‡
    if len(ground_truth_tokens) == 0:
        match_rate = 0.0
    else:
        matches = sum(1 for o, p in zip(ground_truth_tokens, decoded_tokens) if o == p)
        match_rate = matches / len(ground_truth_tokens)
    
    if verbose:
        status = "âœ“ SUCCESS" if original_text == predicted_text else "âœ— FAILED"
        print(f"      ğŸ“Š ç»“æœ: {status}")
        print(f"         Original : {original_text[:70]}{'...' if len(original_text) > 70 else ''}")
        print(f"         Predicted: {predicted_text[:70]}{'...' if len(predicted_text) > 70 else ''}")
        print(f"         Match Rate: {match_rate:.4f}")
    
    return {
        'original': original_text,
        'predicted': predicted_text,
        'match_rate': match_rate,
        'success': original_text == predicted_text
    }

def test_sentence_all_configs(models, tokenizers, sentence, threshold=0.95, verbose=True, token_log_dir: Optional[str] = None, sentence_tag: Optional[str] = None):
    """
    æµ‹è¯•ä¸€ä¸ªå¥å­åœ¨æ‰€æœ‰18ç§é…ç½®ä¸‹çš„åŒ¹é…ç‡
    å¦‚æœæŸä¸ªé…ç½®çš„åŒ¹é…ç‡ä½äºé˜ˆå€¼ï¼Œç«‹å³è¿”å›False
    è¿”å›: (æ˜¯å¦å…¨éƒ¨é€šè¿‡, æ‰€æœ‰é…ç½®çš„ç»“æœ, å¤±è´¥çš„é…ç½®å)
    """
    results = {}
    config_count = 0
    total_configs = 18  # 3 models * 2 perm_types * 3 layers
    
    for model_name in ["llama3-8B", "llama3.2-1B", "qwen3-8B"]:
        model = models[model_name]['model']
        tokenizer = tokenizers[model_name]
        
        for perm_type in ["None", "D"]:
            config_file = "kv_attack_config_no_perm.json" if perm_type == "None" else "kv_attack_config_with_perm.json"
            perm_key = "no_perm" if perm_type == "None" else "with_perm"
            layers = MODEL_CONFIGS[model_name]["layers"][perm_key]
            
            for layer in layers:
                config_count += 1
                config_key = f"{model_name}_{perm_type}_layer{layer}"
                
                print(f"\n    ===== é…ç½® {config_count}/{total_configs}: {config_key} =====")
                logger.info(f"    æµ‹è¯•é…ç½® [{config_count}/{total_configs}]: {config_key}")
                
                matching_eps = load_config(config_file, model_name, layer)
                print(f"    ä½¿ç”¨ matching_eps: {matching_eps:.6f}")
                
                try:
                    token_log_path = None
                    if token_log_dir:
                        # sentence_tag ç”¨äºåŒºåˆ†ä¸åŒå¥å­ï¼ˆä¾‹å¦‚ tested_countï¼‰ï¼Œé¿å…è¦†ç›–
                        tag = sentence_tag or "sentence"
                        token_log_path = os.path.join(token_log_dir, f"{tag}__{config_key}.jsonl")
                    result = run_single_attack(
                        model,
                        tokenizer,
                        sentence,
                        layer,
                        perm_type,
                        matching_eps,
                        verbose=verbose,
                        token_log_path=token_log_path,
                    )
                    results[config_key] = result
                    
                    logger.info(f"      match_rate={result['match_rate']:.4f}, eps={matching_eps:.4f}")
                    if result.get('aborted_due_to_eps'):
                        print(f"\n    â­ï¸  æå‰ç»ˆæ­¢å¥å­ï¼š{config_key} è¯¯å·®è¶…è¿‡ matching_epsï¼Œç›´æ¥åˆ‡æ¢ä¸‹ä¸€ä¸ªå¥å­")
                        logger.warning(f"    â­ï¸  å¥å­æå‰ç»ˆæ­¢ï¼š{config_key} è¯¯å·®è¶…è¿‡ matching_eps")
                        return False, results, config_key
                    
                    if result['match_rate'] < threshold:
                        print(f"\n    âŒâŒâŒ å¤±è´¥äº {config_key}: match_rate={result['match_rate']:.4f} < {threshold}")
                        logger.warning(f"    âŒ å¤±è´¥äº {config_key}: match_rate={result['match_rate']:.4f} < {threshold}")
                        return False, results, config_key
                    else:
                        print(f"    âœ… é…ç½® {config_key} é€šè¿‡ (match_rate={result['match_rate']:.4f} >= {threshold})")
                except Exception as e:
                    print(f"\n    âŒâŒâŒ æ”»å‡»å‡ºé”™äº {config_key}: {str(e)}")
                    logger.error(f"    âŒ æ”»å‡»å‡ºé”™äº {config_key}: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    return False, results, config_key
    
    print(f"\n    ğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰ {total_configs} ä¸ªé…ç½®å…¨éƒ¨é€šè¿‡ï¼")
    return True, results, None

def load_lmsys_data(data_dir):
    """ä»lmsysæ•°æ®é›†åŠ è½½è‹±æ–‡æ•°æ®"""
    parquet_files = sorted(glob.glob(os.path.join(data_dir, "data", "*.parquet")))
    
    for pf in parquet_files:
        logger.info(f"è¯»å–æ–‡ä»¶: {pf}")
        df = pd.read_parquet(pf)
        english_df = df[df['language'] == 'English']
        
        for idx, row in english_df.iterrows():
            try:
                conversation = row['conversation']
                if conversation is not None and len(conversation) > 0:
                    first_msg = conversation[0]
                    if isinstance(first_msg, dict):
                        user_message = first_msg.get('content', '')
                    else:
                        continue
                    if user_message and len(user_message) > 20:
                        yield user_message
            except Exception:
                continue

def truncate_prompt(tokenizer, text: str, num_tokens: int) -> str:
    """æˆªæ–­promptåˆ°æŒ‡å®šçš„tokenæ•°é‡"""
    token_ids = tokenizer.encode(text, add_special_tokens=False)[:num_tokens]
    return tokenizer.decode(token_ids, skip_special_tokens=True)

def main():
    parser = argparse.ArgumentParser(description="æ”¶é›†é«˜è´¨é‡å¥å­")
    parser.add_argument("--target", type=int, default=100, help="ç›®æ ‡å¥å­æ•°é‡")
    parser.add_argument("--threshold", type=float, default=0.95, help="åŒ¹é…ç‡é˜ˆå€¼")
    parser.add_argument("--max_tokens", type=int, default=50, help="æ¯ä¸ªå¥å­çš„æœ€å¤§tokenæ•°")
    parser.add_argument("--output", type=str, default="high_quality_sentences.json", help="è¾“å‡ºæ–‡ä»¶ï¼ˆç¬¦åˆæ¡ä»¶çš„å¥å­ï¼‰")
    parser.add_argument("--results_output", type=str, default="sentence_attack_results.json", help="è¾“å‡ºæ–‡ä»¶ï¼ˆ18ç§é…ç½®çš„è¯¦ç»†ç»“æœï¼‰")
    parser.add_argument("--skip_existing", action="store_true", help="è·³è¿‡ä»å·²æœ‰ç»“æœæ”¶é›†ï¼Œç›´æ¥ä»æ•°æ®é›†å¼€å§‹")
    parser.add_argument("--verbose", action="store_true", default=True, help="æ˜¾ç¤ºè¯¦ç»†æ”»å‡»è¿‡ç¨‹")
    parser.add_argument("--quiet", action="store_true", help="é™é»˜æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºè¯¦ç»†æ”»å‡»è¿‡ç¨‹")
    parser.add_argument("--token_log_dir", type=str, default="token_logs",
                        help="é€tokenå®æ—¶æ—¥å¿—ç›®å½•ï¼ˆJSONLï¼‰ï¼›ä¸ºç©ºåˆ™ä¸è®°å½•")
    args = parser.parse_args()
    
    # å¤„ç†verboseå‚æ•°
    verbose = not args.quiet
    
    set_seed(RANDOM_SEED)
    
    logger.info("=" * 80)
    logger.info("æ”¶é›†é«˜è´¨é‡å¥å­")
    logger.info(f"ç›®æ ‡æ•°é‡: {args.target}")
    logger.info(f"åŒ¹é…ç‡é˜ˆå€¼: {args.threshold}")
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    logger.info(f"è¯¦ç»†ç»“æœæ–‡ä»¶: {args.results_output}")
    logger.info("=" * 80)
    
    # åŠ è½½ä¸€ä¸ªtokenizerç”¨äºè®¡ç®—åŒ¹é…ç‡
    base_tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIGS["llama3-8B"]["path"])
    
    # å·²æ”¶é›†çš„å¥å­
    collected_texts = set()  # ç”¨äºå»é‡
    collected_count = 0
    
    # æ£€æŸ¥å¹¶åŠ è½½å·²æœ‰çš„è¾“å‡ºæ–‡ä»¶ï¼ˆé˜²æ­¢é‡å¤æ”¶é›†ï¼‰
    if os.path.exists(args.output):
        logger.info(f"å‘ç°å·²æœ‰è¾“å‡ºæ–‡ä»¶: {args.output}ï¼Œæ­£åœ¨åŠ è½½...")
        try:
            with open(args.output, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
            
            existing_sentences = existing_data.get('sentences', [])
            for sent_data in existing_sentences:
                sentence = sent_data.get('sentence', '')
                if sentence:
                    collected_texts.add(sentence)
                    collected_count = max(collected_count, sent_data.get('idx', 0))
            
            logger.info(f"å·²åŠ è½½ {len(existing_sentences)} ä¸ªå·²æœ‰å¥å­ï¼Œå½“å‰è®¡æ•°: {collected_count}")
        except Exception as e:
            logger.warning(f"åŠ è½½å·²æœ‰æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°å¼€å§‹")
            collected_texts = set()
            collected_count = 0
    
    # åˆå§‹åŒ–æˆ–æ›´æ–°è¾“å‡ºæ–‡ä»¶
    if collected_count == 0:
        # å¦‚æœæ²¡æœ‰ä»»ä½•å·²æœ‰æ•°æ®ï¼Œåˆå§‹åŒ–æ–‡ä»¶
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump({'sentences': [], 'total_count': 0, 'target': args.target, 'threshold': args.threshold}, f, indent=2, ensure_ascii=False)
        
        with open(args.results_output, 'w', encoding='utf-8') as f:
            json.dump({'sentence_results': []}, f, indent=2, ensure_ascii=False)
    else:
        logger.info(f"å°†ä»å·²æœ‰è¿›åº¦ç»§ç»­ï¼šå·²æ”¶é›† {collected_count} ä¸ªå¥å­ï¼Œç›®æ ‡ {args.target} ä¸ª")
    
    # 1. å…ˆä»å·²æœ‰ç»“æœä¸­æ”¶é›†
    if not args.skip_existing:
        logger.info("\n" + "=" * 40)
        logger.info("æ­¥éª¤1: ä»å·²æœ‰æ”»å‡»ç»“æœä¸­æ”¶é›†ç¬¦åˆæ¡ä»¶çš„å¥å­")
        logger.info("=" * 40)
        
        qualified = load_existing_qualified_sentences(base_tokenizer, args.threshold)
        
        for item in qualified:
            if collected_count >= args.target:
                break
            
            sentence = item['sentence']
            if sentence in collected_texts:
                continue
            
            collected_texts.add(sentence)
            collected_count += 1
            
            # ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶
            save_qualified_sentence(args.output, {
                'idx': collected_count,
                'sentence': sentence,
                'source': 'existing_results',
                'sample_idx': item['sample_idx'],
                'match_rates': item['match_rates']
            })
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            save_sentence_results(args.results_output, collected_count, sentence, item['results'])
            
            logger.info(f"âœ… æ”¶é›†å¥å­ #{collected_count}: {sentence[:60]}...")
        
        logger.info(f"ä»å·²æœ‰ç»“æœä¸­æ”¶é›†åˆ° {collected_count} ä¸ªç¬¦åˆæ¡ä»¶çš„è‹±æ–‡å¥å­")
    
    # 2. å¦‚æœä¸å¤Ÿï¼Œä»lmsysæ•°æ®é›†ç»§ç»­é‡‡æ ·
    if collected_count < args.target:
        logger.info("\n" + "=" * 40)
        logger.info(f"æ­¥éª¤2: ä»lmsysæ•°æ®é›†é‡‡æ ·ï¼Œè¿˜éœ€è¦ {args.target - collected_count} ä¸ªå¥å­")
        logger.info("=" * 40)
        
        logger.info("åŠ è½½æ¨¡å‹...")
        device_map = "cuda"
        model_dtype = torch.bfloat16
        
        models = {}
        tokenizers = {}
        
        for model_name, config in MODEL_CONFIGS.items():
            logger.info(f"  åŠ è½½ {model_name}...")
            tokenizers[model_name] = AutoTokenizer.from_pretrained(config["path"])
            models[model_name] = {
                'model': AutoModelForCausalLM.from_pretrained(
                    config["path"], 
                    torch_dtype=model_dtype, 
                    attn_implementation="eager"
                ).to(device_map)
            }
            models[model_name]['model'].eval()
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        data_dir = "/home/junjie_chen/datasets/lmsys-chat-1m-data"
        tested_count = 0
        
        for sentence in load_lmsys_data(data_dir):
            if collected_count >= args.target:
                break
            
            # æˆªæ–­å¥å­
            sentence = truncate_prompt(base_tokenizer, sentence, args.max_tokens)
            
            # æ£€æŸ¥æ˜¯å¦é‡å¤
            if sentence in collected_texts:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è‹±æ–‡
            if not is_english(sentence):
                continue
            
            # æ£€æŸ¥é•¿åº¦
            if len(sentence) < 20:
                continue
            
            tested_count += 1
            print("\n" + "=" * 80)
            print(f"ğŸ” æµ‹è¯•å¥å­ #{tested_count} (å·²æ”¶é›†: {collected_count}/{args.target})")
            print("=" * 80)
            print(f"ğŸ“ å¥å­å†…å®¹: {sentence}")
            print(f"ğŸ“ å¥å­é•¿åº¦: {len(sentence)} å­—ç¬¦")
            logger.info(f"æµ‹è¯•å¥å­ #{tested_count} (å·²æ”¶é›†: {collected_count}/{args.target})")
            logger.info(f"  å¥å­: {sentence[:80]}{'...' if len(sentence) > 80 else ''}")
            
            # æµ‹è¯•æ‰€æœ‰é…ç½®
            qualified, results, failed_config = test_sentence_all_configs(
                models,
                tokenizers,
                sentence,
                args.threshold,
                verbose=verbose,
                token_log_dir=(args.token_log_dir if args.token_log_dir else None),
                sentence_tag=f"tested_{tested_count}_collected_{collected_count}",
            )
            
            if qualified:
                collected_count += 1
                collected_texts.add(sentence)
                
                match_rates = {k: v['match_rate'] for k, v in results.items()}
                min_rate = min(match_rates.values())
                mean_rate = sum(match_rates.values()) / len(match_rates)
                
                # ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶
                save_qualified_sentence(args.output, {
                    'idx': collected_count,
                    'sentence': sentence,
                    'source': 'lmsys_dataset',
                    'match_rates': match_rates,
                    'min_match_rate': min_rate,
                    'mean_match_rate': mean_rate
                })
                
                # ä¿å­˜è¯¦ç»†ç»“æœ
                save_sentence_results(args.results_output, collected_count, sentence, results)
                
                print("\n" + "ğŸŠ" * 20)
                print(f"âœ…âœ…âœ… å¥å­ #{collected_count} åˆæ ¼ï¼å·²ä¿å­˜åˆ°JSON")
                print(f"    æœ€å°åŒ¹é…ç‡: {min_rate:.4f}")
                print(f"    å¹³å‡åŒ¹é…ç‡: {mean_rate:.4f}")
                print(f"    å½“å‰è¿›åº¦: {collected_count}/{args.target}")
                print("ğŸŠ" * 20)
                logger.info(f"  âœ… å¥å­åˆæ ¼ï¼(min={min_rate:.4f}, mean={mean_rate:.4f})")
                logger.info(f"  å½“å‰è¿›åº¦: {collected_count}/{args.target}")
            else:
                print("\n" + "âŒ" * 20)
                print(f"âŒ å¥å­ä¸åˆæ ¼ï¼Œå¤±è´¥äº: {failed_config}")
                print("âŒ" * 20)
                logger.info(f"  âŒ å¥å­ä¸åˆæ ¼ï¼Œå¤±è´¥äº: {failed_config}")
    
    logger.info("\n" + "=" * 80)
    logger.info(f"æ”¶é›†å®Œæˆï¼å…±æ”¶é›† {collected_count} ä¸ªé«˜è´¨é‡å¥å­")
    logger.info(f"ç¬¦åˆæ¡ä»¶çš„å¥å­å·²ä¿å­˜åˆ°: {args.output}")
    logger.info(f"è¯¦ç»†æ”»å‡»ç»“æœå·²ä¿å­˜åˆ°: {args.results_output}")
    logger.info("=" * 80)

if __name__ == "__main__":
    main()
