# Tokenizer ç‰¹æ®Šç¬¦å·æ£€æŸ¥æŠ¥å‘Š

## âœ… æ£€æŸ¥ç»“æœï¼šå…¨éƒ¨æ­£ç¡®

### ğŸ“‹ æ£€æŸ¥é¡¹ç›®

#### 1. TOKENIZER.encode (ç¼–ç )

**ä½ç½® 1: gen_hidden_states() - ç¬¬ 58 è¡Œ**
```python
token_ids = TOKENIZER.encode(
    sentence, 
    return_tensors="pt", 
    add_special_tokens=False  # âœ… æ­£ç¡®
).to(device_map)
```
- âœ… ä¸æ·»åŠ  BOS/EOS token
- âœ… ç”¨äºç”Ÿæˆæ”»å‡»ç›®æ ‡çš„éšè—çŠ¶æ€

**ä½ç½® 2: truncate_prompt() - ç¬¬ 697 è¡Œ**
```python
token_ids = TOKENIZER.encode(
    text, 
    add_special_tokens=False  # âœ… æ­£ç¡®
)[:num_tokens]
```
- âœ… ä¸æ·»åŠ ç‰¹æ®Š token
- âœ… ç”¨äºæˆªæ–­æ–‡æœ¬åˆ°æŒ‡å®š token æ•°

**ä½ç½® 3: ç»Ÿè®¡ token æ•°é‡ - ç¬¬ 816 è¡Œ**
```python
'num_tokens': len(TOKENIZER.encode(
    prompt, 
    add_special_tokens=False  # âœ… æ­£ç¡®
))
```
- âœ… ä¸æ·»åŠ ç‰¹æ®Š token
- âœ… ç”¨äºç»Ÿè®¡å®é™… token æ•°é‡

#### 2. TOKENIZER.decode (è§£ç )

**ä½ç½® 1: truncate_prompt() - ç¬¬ 698 è¡Œ**
```python
return TOKENIZER.decode(
    token_ids, 
    skip_special_tokens=True  # âœ… æ­£ç¡®
)
```
- âœ… è·³è¿‡ç‰¹æ®Š token
- âœ… è¿”å›çº¯æ–‡æœ¬

**ä½ç½® 2: æ”»å‡»ç»“æœè§£ç  - ç¬¬ 806 è¡Œ**
```python
obtained_prediction = TOKENIZER.decode(
    decoded_tokens, 
    skip_special_tokens=True  # âœ… æ­£ç¡®
)
```
- âœ… è·³è¿‡ç‰¹æ®Š token
- âœ… è§£ç æ”»å‡»æ¢å¤çš„ token

## ğŸ¯ æ€»ç»“

### âœ… å…¨éƒ¨é€šè¿‡
- **3ä¸ª encode ä½ç½®**: å…¨éƒ¨ä½¿ç”¨ `add_special_tokens=False`
- **2ä¸ª decode ä½ç½®**: å…¨éƒ¨ä½¿ç”¨ `skip_special_tokens=True`

### ğŸ”’ ä¿è¯
1. âŒ **ä¸ä¼šæ·»åŠ ** BOS (Beginning of Sentence) token
2. âŒ **ä¸ä¼šæ·»åŠ ** EOS (End of Sentence) token  
3. âŒ **ä¸ä¼šæ·»åŠ ** PAD (Padding) token
4. âœ… **åªå¤„ç†** å®é™…æ–‡æœ¬å†…å®¹çš„ token

### ğŸ“Š ç¤ºä¾‹

#### Llama-3 çš„ç‰¹æ®Š token
```python
BOS token: <|begin_of_text|>  (ID = 128000)
EOS token: <|end_of_text|>    (ID = 128001)
```

#### æˆ‘ä»¬çš„ç¼–ç æ–¹å¼
```python
# âŒ é”™è¯¯ï¼ˆä¼šæ·»åŠ ç‰¹æ®Š tokenï¼‰
tokens = tokenizer.encode("Hello world")
# ç»“æœ: [128000, 9906, 1917]
#       â†‘ BOS token

# âœ… æ­£ç¡®ï¼ˆä¸æ·»åŠ ç‰¹æ®Š tokenï¼‰
tokens = tokenizer.encode("Hello world", add_special_tokens=False)
# ç»“æœ: [9906, 1917]
#       â†‘ ç›´æ¥æ˜¯ "Hello"
```

### ğŸ” éªŒè¯æ–¹æ³•

å¦‚æœæƒ³æ‰‹åŠ¨éªŒè¯ï¼š
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/home/junjie_chen/models/llama3-8B")

text = "The history of Nero"

# å¸¦ç‰¹æ®Š token
with_special = tokenizer.encode(text)
print("å¸¦ç‰¹æ®Štoken:", with_special)
print("è§£ç :", tokenizer.decode(with_special))

# ä¸å¸¦ç‰¹æ®Š token
without_special = tokenizer.encode(text, add_special_tokens=False)
print("ä¸å¸¦ç‰¹æ®Štoken:", without_special)
print("è§£ç :", tokenizer.decode(without_special))
```

## âœ… ç»“è®º

**æ‰€æœ‰ tokenizer è°ƒç”¨éƒ½å·²æ­£ç¡®é…ç½®ï¼Œä¸ä¼šå¼•å…¥ BOSã€EOS ç­‰ç‰¹æ®Šç¬¦å·ï¼**

æ”»å‡»è¿‡ç¨‹ä¸­åªå¤„ç†å®é™…çš„æ–‡æœ¬ tokenï¼Œç¡®ä¿äº†ï¼š
1. ç›®æ ‡éšè—çŠ¶æ€ = çº¯æ–‡æœ¬çš„éšè—çŠ¶æ€
2. è¯æ±‡è¡¨éšè—çŠ¶æ€ = çº¯æ–‡æœ¬çš„éšè—çŠ¶æ€
3. åŒ¹é…è¿‡ç¨‹ = åœ¨ç›¸åŒç©ºé—´ä¸­è¿›è¡Œ
4. ç»“æœè§£ç  = çº¯å‡€çš„æ–‡æœ¬å†…å®¹

---

**æ£€æŸ¥æ—¶é—´**: 2025-12-22  
**æ£€æŸ¥çŠ¶æ€**: âœ… é€šè¿‡  
**å»ºè®®**: ä¿æŒå½“å‰é…ç½®

