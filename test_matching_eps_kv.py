import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
import numpy as np
import random
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
from tqdm import tqdm
import argparse

# Set random seeds for reproducibility
RANDOM_SEED = 42

def set_seed(seed=RANDOM_SEED):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Model configurations
MODEL_CONFIGS = {
    "llama3.2-1B": "/home/junjie_chen/models/llama3.2-1B",
    "llama3-8B": "/home/junjie_chen/models/llama3-8B",
    # "llama-7B": "/home/junjie_chen/models/llama-7B",  # æš‚æ—¶ç§»é™¤ï¼štorchç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
    "qwen3-8B": "/home/junjie_chen/models/qwen3-8B",
}

def gen_kv_states(model, tokenizer, sentence, layers=[1], device_map="cuda"):
    """
    ç”ŸæˆæŒ‡å®šå±‚çš„Kå’ŒV states
    
    Returns:
        tuple: (k_states_list, v_states_list)
    """
    token_ids = tokenizer.encode(sentence, return_tensors="pt", add_special_tokens=False).to(device_map)
    
    with torch.no_grad():
        outputs = model(token_ids, use_cache=True, output_hidden_states=True)
    
    k_states_list = []
    v_states_list = []
    
    for layer_idx in layers:
        k_cache = outputs.past_key_values[layer_idx][0]
        v_cache = outputs.past_key_values[layer_idx][1]
        
        batch_size, num_heads, seq_len, head_dim = k_cache.shape
        k_states = k_cache.squeeze(0).transpose(0, 1).reshape(seq_len, num_heads * head_dim)
        v_states = v_cache.squeeze(0).transpose(0, 1).reshape(seq_len, num_heads * head_dim)
        
        k_states_list.append(k_states)
        v_states_list.append(v_states)
    
    return k_states_list, v_states_list

def generate_permutation(N: int, d: int, perm_type: str, device: torch.device) -> tuple:
    """ç”Ÿæˆç½®æ¢ç´¢å¼•ï¼Œç”¨äºKå’ŒVå…±äº«åŒä¸€ç½®æ¢"""
    seq_perm = None
    dim_perm = None
    
    if perm_type == "S":
        seq_perm = torch.randperm(N, device=device)
    elif perm_type == "D":
        dim_perm = torch.randperm(d, device=device)
    elif perm_type == "SD":
        seq_perm = torch.randperm(N, device=device)
        dim_perm = torch.randperm(d, device=device)
    
    return seq_perm, dim_perm

def apply_permutation(states: torch.Tensor, seq_perm: torch.Tensor, dim_perm: torch.Tensor) -> torch.Tensor:
    """åº”ç”¨é¢„ç”Ÿæˆçš„ç½®æ¢"""
    result = states
    if dim_perm is not None:
        result = result[:, dim_perm]
    if seq_perm is not None:
        result = result[seq_perm]
    return result

def permute_states(states: torch.Tensor, perm_type: str) -> torch.Tensor:
    """å¯¹statesè¿›è¡Œç½®æ¢ï¼ˆå•ç‹¬ä½¿ç”¨æ—¶ï¼Œä¸ä¸å…¶ä»–tensorå…±äº«ç½®æ¢ï¼‰"""
    N, d = states.size()
    device = states.device
    if perm_type == "None":
        return states
    elif perm_type == "S":
        return states[torch.randperm(N, device=device)]
    elif perm_type == "D":
        return states[:, torch.randperm(d, device=device)]
    elif perm_type == "SD":
        return permute_states(permute_states(states, "D"), "S")
    else:
        raise Exception(f"Unsupported permutation pattern {perm_type}")

def compute_kv_errors(
    model,
    tokenizer,
    sentence: str,
    layer: int,
    perm_type: str = "None",
    device_map: str = "cuda",
) -> list[dict]:
    """
    ç›´æ¥ä½¿ç”¨æ­£ç¡®çš„tokenåºåˆ—ï¼Œè®¡ç®—æ¯ä¸ªtokençš„Kå’ŒVçš„matching error
    
    Args:
        model: æ¨¡å‹
        tokenizer: tokenizer
        sentence: è¾“å…¥å¥å­
        layer: è¦æµ‹è¯•çš„å±‚
        perm_type: ç½®æ¢ç±»å‹
        device_map: è®¾å¤‡
    
    Returns:
        æ¯ä¸ªtokençš„errorä¿¡æ¯åˆ—è¡¨
    """
    # è·å–ground truth tokens
    ground_truth_tokens = tokenizer.encode(sentence, add_special_tokens=False)
    num_tokens = len(ground_truth_tokens)
    
    # ç”Ÿæˆpermuted Kå’ŒV states
    k_states_list, v_states_list = gen_kv_states(model, tokenizer, sentence, layers=[layer], device_map=device_map)
    k_states = k_states_list[0]
    v_states = v_states_list[0]
    
    # ç”Ÿæˆå…±äº«çš„ç½®æ¢ï¼ˆKå’ŒVä½¿ç”¨ç›¸åŒçš„ç½®æ¢ï¼‰
    N, d = k_states.size()
    seq_perm, dim_perm = generate_permutation(N, d, perm_type, k_states.device)
    
    # åº”ç”¨ç›¸åŒçš„ç½®æ¢åˆ°Kå’ŒV
    perm_k_states = apply_permutation(k_states, seq_perm, dim_perm)
    perm_v_states = apply_permutation(v_states, seq_perm, dim_perm)
    
    # è®°å½•æ¯ä¸ªtokençš„error
    error_logs = []
    
    # æ ¹æ® perm_type å†³å®šæ˜¯å¦ä½¿ç”¨æ’åº
    use_sort = (perm_type == "D" or perm_type == "SD")
    
    # é€æ­¥æ„å»ºtokenåºåˆ—ï¼Œæ¯æ¬¡æ·»åŠ ä¸€ä¸ªæ­£ç¡®çš„token
    for i in range(num_tokens):
        current_tokens = ground_truth_tokens[:i]
        next_token = ground_truth_tokens[i]
        
        if i == 0:
            input_ids = torch.tensor([[next_token]], device=device_map)
        else:
            input_ids = torch.tensor([current_tokens + [next_token]], device=device_map)
        
        # Forward passè·å–Kå’ŒV states
        with torch.no_grad():
            outputs = model(input_ids, use_cache=True, output_hidden_states=True)
        
        # æå–æœ€åä¸€ä¸ªtokençš„Kå’ŒV
        k_cache = outputs.past_key_values[layer][0]
        v_cache = outputs.past_key_values[layer][1]
        
        batch_size, num_heads, seq_len, head_dim = k_cache.shape
        k_last = k_cache[:, :, -1, :].reshape(num_heads * head_dim)
        v_last = v_cache[:, :, -1, :].reshape(num_heads * head_dim)
        
        # è®¡ç®—L1è·ç¦»
        perm_k_row = perm_k_states[i, :]
        perm_v_row = perm_v_states[i, :]
        
        # æ ¹æ® perm_type å†³å®šæ˜¯å¦æ’åº
        if use_sort:
            sorted_perm_k, _ = torch.sort(perm_k_row)
            sorted_k, _ = torch.sort(k_last)
            sorted_perm_v, _ = torch.sort(perm_v_row)
            sorted_v, _ = torch.sort(v_last)
        else:
            sorted_perm_k = perm_k_row
            sorted_k = k_last
            sorted_perm_v = perm_v_row
            sorted_v = v_last
        
        k_error = torch.sum(torch.abs(sorted_perm_k - sorted_k)).item()
        v_error = torch.sum(torch.abs(sorted_perm_v - sorted_v)).item()
        
        total_error = k_error + v_error
        
        token_log = {
            'token_index': i,
            'token_id': next_token,
            'token_text': tokenizer.decode([next_token]),
            'k_error': k_error,
            'v_error': v_error,
            'total_error': total_error,
        }
        error_logs.append(token_log)
        
        if (i + 1) % 10 == 0:
            print(f"  Token {i+1}/{num_tokens}: '{tokenizer.decode([next_token])}', "
                  f"k_err={k_error:.4f}, v_err={v_error:.4f}, total={total_error:.4f}")
        
        del outputs
        torch.cuda.empty_cache()
    
    return error_logs

def truncate_prompt(tokenizer, text: str, num_tokens: int) -> str:
    """æˆªæ–­promptåˆ°æŒ‡å®šçš„tokenæ•°é‡"""
    token_ids = tokenizer.encode(text, add_special_tokens=False)[:num_tokens]
    return tokenizer.decode(token_ids, skip_special_tokens=True)

def load_lmsys_samples(tokenizer, data_file, num_samples=100, max_tokens=50, seed=RANDOM_SEED):
    """ä»lmsysæ•°æ®é›†åŠ è½½æ ·æœ¬"""
    print(f"Loading samples from {data_file} (seed={seed})...")
    
    with open(data_file, 'r') as f:
        all_data = json.load(f)
    
    all_valid_samples = []
    for text in all_data:
        text = text.strip()
        if len(text) > 20:
            truncated = truncate_prompt(tokenizer, text, max_tokens)
            if len(truncated) > 10:
                all_valid_samples.append(truncated)
    
    print(f"Found {len(all_valid_samples)} valid samples")
    
    if len(all_valid_samples) > num_samples:
        random.seed(seed)
        samples = random.sample(all_valid_samples, num_samples)
        print(f"Randomly sampled {num_samples} samples (seed={seed})")
    else:
        samples = all_valid_samples[:num_samples]
        print(f"Using first {len(samples)} samples")
    
    return samples

def print_layer_recommendations(layer, all_total_errors, num_samples, total_tokens):
    """æ‰“å°è¯¥å±‚çš„è¯¦ç»†æ¨èé…ç½®"""
    print(f"\n{'='*80}")
    print(f"Layer {layer} æµ‹è¯•å®Œæˆ - è¯¦ç»†æ¨èé…ç½®")
    print(f"{'='*80}")
    print(f"æ ·æœ¬æ•°: {num_samples}")
    print(f"æ€»Tokenæ•°: {total_tokens}")
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nğŸ“Š Errorç»Ÿè®¡:")
    print(f"  æœ€å°å€¼: {np.min(all_total_errors):.6f}")
    print(f"  æœ€å¤§å€¼: {np.max(all_total_errors):.6f}")
    print(f"  å¹³å‡å€¼: {np.mean(all_total_errors):.6f}")
    print(f"  ä¸­ä½æ•°: {np.median(all_total_errors):.6f}")
    print(f"  æ ‡å‡†å·®: {np.std(all_total_errors):.6f}")
    
    # ç™¾åˆ†ä½æ•°ç»Ÿè®¡
    print(f"\nğŸ“ˆ ç™¾åˆ†ä½æ•°åˆ†å¸ƒ:")
    percentiles = [50, 75, 80, 85, 90, 95, 99, 99.5, 99.9]
    for p in percentiles:
        val = np.percentile(all_total_errors, p)
        print(f"  {p:5.1f}%: {val:.6f}  (è¦†ç›– {p:.1f}% çš„token)")
    
    # æ¨èé…ç½®ï¼ˆä»¥æˆåŠŸç‡ä¼˜å…ˆï¼‰
    print(f"\nğŸ¯ æ¨èçš„matching_epsé…ç½® (æŒ‰æˆåŠŸç‡ä¼˜å…ˆ):")
    print(f"{'ç­–ç•¥':<20} {'matching_eps':<15} {'è¦†ç›–ç‡':<15} {'è¯´æ˜':<30}")
    print(f"{'-'*80}")
    
    recommendations = [
        ("æè‡´æˆåŠŸç‡", 99.9, "å‡ ä¹100%æˆåŠŸ"),
        ("éå¸¸ä¿å®ˆ", 99.5, "99.5%ä»¥ä¸ŠæˆåŠŸ"),
        ("ä¿å®ˆ", 99, "99%ä»¥ä¸ŠæˆåŠŸ"),
        ("æ¨è(å¹³è¡¡)", 95, "95%ä»¥ä¸ŠæˆåŠŸï¼Œæ¨èä½¿ç”¨"),
        ("æ¿€è¿›", 90, "90%ä»¥ä¸ŠæˆåŠŸ"),
        ("éå¸¸æ¿€è¿›", 85, "85%ä»¥ä¸ŠæˆåŠŸ"),
        ("é«˜é£é™©", 80, "80%ä»¥ä¸ŠæˆåŠŸï¼Œå¯èƒ½å¤±è´¥è¾ƒå¤š"),
        ("å®éªŒæ€§", 75, "75%ä»¥ä¸ŠæˆåŠŸï¼Œä»…ä¾›å®éªŒ"),
    ]
    
    for strategy, percentile, desc in recommendations:
        eps_value = np.percentile(all_total_errors, percentile)
        print(f"{strategy:<20} {eps_value:<15.6f} {percentile:<15.1f}%  {desc:<30}")
    
    # é»˜è®¤æ¨è
    recommended_eps = np.percentile(all_total_errors, 95)
    print(f"\nâ­ é»˜è®¤æ¨è: {recommended_eps:.6f} (95%åˆ†ä½ï¼Œå¹³è¡¡æˆåŠŸç‡å’Œä¸¥æ ¼æ€§)")
    print(f"{'='*80}\n")

def test_model(model_name: str, model_path: str, num_samples: int, max_tokens: int, perm_type: str = "None"):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹å¹¶è¿”å›æ¨èé…ç½®"""
    
    print(f"\n{'='*80}")
    print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ç½®æ¢ç±»å‹: {perm_type}")
    print(f"{'='*80}\n")
    
    # åŠ è½½æ¨¡å‹
    device_map = "cuda"
    model_dtype = torch.bfloat16
    
    print(f"Loading model...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path, torch_dtype=model_dtype, attn_implementation="eager"
    ).to(device_map)
    model.eval()
    
    num_hidden_layers = model.config.num_hidden_layers
    print(f"Model loaded. Number of layers: {num_hidden_layers}")
    
    # ç¡®å®šè¦æµ‹è¯•çš„å±‚
    # ä»layer 0å¼€å§‹ï¼ˆç¬¬ä¸€å±‚ï¼‰
    first_layer = 0
    middle_layer = num_hidden_layers // 2
    last_layer = num_hidden_layers - 1
    
    test_layers = [first_layer, middle_layer, last_layer]
    
    print(f"\nTesting layers: {test_layers}")
    print(f"  - First layer: {first_layer} (layer 0)")
    print(f"  - Middle layer: {middle_layer}")
    print(f"  - Last layer: {last_layer}")
    
    # åŠ è½½æ•°æ®
    data_file = "/home/junjie_chen/datasets/lmsys-chat-1m-data-1000/data.json"
    test_samples = load_lmsys_samples(tokenizer, data_file, num_samples=num_samples, 
                                      max_tokens=max_tokens, seed=RANDOM_SEED)
    
    print(f"\nLoaded {len(test_samples)} samples\n")
    
    # å¯¹æ¯ä¸€å±‚è¿è¡Œæµ‹è¯•
    layer_results = {}
    
    for layer in test_layers:
        print(f"\n{'='*80}")
        print(f"Testing Layer {layer}")
        print(f"{'='*80}\n")
        
        sample_results = []
        
        for idx, prompt in enumerate(tqdm(test_samples, desc=f"Layer {layer}")):
            try:
                error_logs = compute_kv_errors(model, tokenizer, prompt, layer, perm_type, device_map)
                
                total_errors = [log['total_error'] for log in error_logs]
                k_errors = [log['k_error'] for log in error_logs]
                v_errors = [log['v_error'] for log in error_logs]
                
                sample_result = {
                    'sample_idx': idx,
                    'text': prompt,
                    'num_tokens': len(error_logs),
                    'error_statistics': {
                        'total': {
                            'min': float(np.min(total_errors)),
                            'max': float(np.max(total_errors)),
                            'mean': float(np.mean(total_errors)),
                            'median': float(np.median(total_errors)),
                            'std': float(np.std(total_errors)),
                        },
                        'k': {
                            'min': float(np.min(k_errors)),
                            'max': float(np.max(k_errors)),
                            'mean': float(np.mean(k_errors)),
                            'median': float(np.median(k_errors)),
                        },
                        'v': {
                            'min': float(np.min(v_errors)),
                            'max': float(np.max(v_errors)),
                            'mean': float(np.mean(v_errors)),
                            'median': float(np.median(v_errors)),
                        }
                    },
                    'error_logs': error_logs
                }
                
                sample_results.append(sample_result)
            
            except Exception as e:
                print(f"\nâŒ Error processing sample {idx}: {e}")
                import traceback
                traceback.print_exc()
        
        # è®¡ç®—è¯¥å±‚çš„å…¨å±€ç»Ÿè®¡
        all_total_errors = []
        all_k_errors = []
        all_v_errors = []
        
        for result in sample_results:
            if 'error_logs' in result:
                for log in result['error_logs']:
                    all_total_errors.append(log['total_error'])
                    all_k_errors.append(log['k_error'])
                    all_v_errors.append(log['v_error'])
        
        # ç«‹å³æ‰“å°è¯¥å±‚çš„æ¨èé…ç½®
        print_layer_recommendations(layer, all_total_errors, len(sample_results), len(all_total_errors))
        
        layer_statistics = {
            'layer': layer,
            'num_samples': len(sample_results),
            'total_tokens': len(all_total_errors),
            'global_statistics': {
                'total_error': {
                    'min': float(np.min(all_total_errors)),
                    'max': float(np.max(all_total_errors)),
                    'mean': float(np.mean(all_total_errors)),
                    'median': float(np.median(all_total_errors)),
                    'std': float(np.std(all_total_errors)),
                    'percentiles': {
                        '50': float(np.percentile(all_total_errors, 50)),
                        '75': float(np.percentile(all_total_errors, 75)),
                        '80': float(np.percentile(all_total_errors, 80)),
                        '85': float(np.percentile(all_total_errors, 85)),
                        '90': float(np.percentile(all_total_errors, 90)),
                        '95': float(np.percentile(all_total_errors, 95)),
                        '99': float(np.percentile(all_total_errors, 99)),
                        '99.5': float(np.percentile(all_total_errors, 99.5)),
                        '99.9': float(np.percentile(all_total_errors, 99.9)),
                    }
                },
                'k_error': {
                    'mean': float(np.mean(all_k_errors)),
                    'median': float(np.median(all_k_errors)),
                },
                'v_error': {
                    'mean': float(np.mean(all_v_errors)),
                    'median': float(np.median(all_v_errors)),
                }
            }
        }
        
        layer_results[f"layer_{layer}"] = layer_statistics
        
        # ç«‹å³ä¿å­˜å½“å‰å±‚çš„ç»“æœåˆ°ä¸´æ—¶æ–‡ä»¶
        temp_output = {
            'model_name': model_name,
            'model_path': model_path,
            'perm_type': perm_type,
            'layer_results': layer_results
        }
        temp_file = f"kv_test_temp_{model_name.replace('.', '_')}_{perm_type}.json"
        with open(temp_file, 'w') as f:
            json.dump(temp_output, f, indent=2)
        print(f"ğŸ’¾ å·²ä¿å­˜ä¸´æ—¶ç»“æœåˆ°: {temp_file}\n")
    
    # æ¸…ç†æ¨¡å‹ä»¥é‡Šæ”¾å†…å­˜
    del model
    del tokenizer
    torch.cuda.empty_cache()
    
    return layer_results

def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•KV cacheæ”»å‡»çš„matching_epsé…ç½®")
    parser.add_argument("--models", nargs="+", default=list(MODEL_CONFIGS.keys()),
                        choices=list(MODEL_CONFIGS.keys()),
                        help="è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨")
    parser.add_argument("--num_samples", type=int, default=100,
                        help="æ¯ä¸ªæ¨¡å‹æµ‹è¯•çš„æ ·æœ¬æ•°")
    parser.add_argument("--max_tokens", type=int, default=50,
                        help="æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§tokenæ•°")
    parser.add_argument("--perm_type", type=str, default="None",
                        choices=["None", "D"],
                        help="ç½®æ¢ç±»å‹: None=æ— ç½®æ¢, D=ç»´åº¦ç½®æ¢")
    parser.add_argument("--output", type=str, default=None,
                        help="è¾“å‡ºé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤æ ¹æ®perm_typeè‡ªåŠ¨å‘½åï¼‰")
    
    args = parser.parse_args()
    
    # æ ¹æ®perm_typeè‡ªåŠ¨è®¾ç½®è¾“å‡ºæ–‡ä»¶å
    if args.output is None:
        if args.perm_type == "None":
            args.output = "kv_attack_config_no_perm.json"
        elif args.perm_type == "D":
            args.output = "kv_attack_config_with_perm.json"
        else:
            args.output = "kv_attack_config.json"
    
    # è®¾ç½®éšæœºç§å­
    set_seed(RANDOM_SEED)
    print(f"Random seed set to: {RANDOM_SEED}")
    
    print(f"\n{'='*80}")
    print(f"KV Cache æ”»å‡»é…ç½®æµ‹è¯•")
    print(f"{'='*80}")
    print(f"æµ‹è¯•æ¨¡å‹: {args.models}")
    print(f"æ ·æœ¬æ•°: {args.num_samples}")
    print(f"æœ€å¤§tokenæ•°: {args.max_tokens}")
    print(f"ç½®æ¢ç±»å‹: {args.perm_type}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    print(f"{'='*80}\n")
    
    # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
    all_results = {}
    
    for model_name in args.models:
        model_path = MODEL_CONFIGS[model_name]
        
        try:
            layer_results = test_model(model_name, model_path, args.num_samples, args.max_tokens, args.perm_type)
            
            # æå–æ¨èé…ç½®ï¼ˆä½¿ç”¨95%åˆ†ä½ä½œä¸ºé»˜è®¤ï¼‰
            recommended_config = {
                'model_name': model_name,
                'model_path': model_path,
                'num_layers': len(layer_results),
                'perm_type': args.perm_type,
                'layers': {}
            }
            
            for layer_key, stats in layer_results.items():
                layer = stats['layer']
                recommended_eps = stats['global_statistics']['total_error']['percentiles']['95']
                
                recommended_config['layers'][str(layer)] = {
                    'matching_eps': recommended_eps,
                    'mean_error': stats['global_statistics']['total_error']['mean'],
                    'median_error': stats['global_statistics']['total_error']['median'],
                    'total_tokens': stats['total_tokens'],
                    'all_percentiles': stats['global_statistics']['total_error']['percentiles']
                }
            
            all_results[model_name] = recommended_config
            
            # æ¯æµ‹è¯•å®Œä¸€ä¸ªæ¨¡å‹å°±ä¿å­˜ä¸€æ¬¡æœ€ç»ˆé…ç½®
            with open(args.output, 'w') as f:
                json.dump(all_results, f, indent=2)
            print(f"ğŸ’¾ å·²ä¿å­˜é…ç½®åˆ°: {args.output}")
            
        except Exception as e:
            print(f"\nâŒ Error testing model {model_name}: {e}")
            import traceback
            traceback.print_exc()
    
    # æœ€ç»ˆä¿å­˜é…ç½®
    with open(args.output, 'w') as f:
        json.dump(all_results, f, indent=2)
    
    print(f"\n{'='*80}")
    print(f"ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print(f"{'='*80}\n")
    
    # æ‰“å°æ¨èé…ç½®æ€»ç»“
    print("æ¨èçš„é…ç½®æ€»ç»“ (95%åˆ†ä½):\n")
    for model_name, config in all_results.items():
        print(f"{model_name}:")
        for layer, layer_config in config['layers'].items():
            print(f"  Layer {layer}: matching_eps = {layer_config['matching_eps']:.6f}")
        print()
    
    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {args.output}")
    print(f"{'='*80}\n")

if __name__ == "__main__":
    main()

