#!/usr/bin/env python3
"""
å°†llama-7Bæ¨¡å‹ä».binæ ¼å¼è½¬æ¢ä¸ºsafetensorsæ ¼å¼
"""

import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import shutil
from pathlib import Path
import argparse

# é…ç½®
SOURCE_MODEL = "/home/junjie_chen/models/llama-7B"
TARGET_MODEL = "/home/junjie_chen/models/llama-7B-safetensors"

# å‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description="å°†llama-7Bè½¬æ¢ä¸ºsafetensorsæ ¼å¼")
parser.add_argument("--device", type=str, default="auto", 
                    choices=["auto", "cuda", "cpu"],
                    help="ä½¿ç”¨çš„è®¾å¤‡: auto(è‡ªåŠ¨é€‰æ‹©), cuda(GPU), cpu")
parser.add_argument("--dtype", type=str, default="float16",
                    choices=["float16", "bfloat16", "float32"],
                    help="æ¨¡å‹ç²¾åº¦: float16, bfloat16, float32")
args = parser.parse_args()

# ç¡®å®šä½¿ç”¨çš„è®¾å¤‡
if args.device == "auto":
    if torch.cuda.is_available():
        device_map = "auto"  # è‡ªåŠ¨åˆ†é…åˆ°GPU
        device_name = "GPU (è‡ªåŠ¨åˆ†é…)"
    else:
        device_map = "cpu"
        device_name = "CPU"
elif args.device == "cuda":
    if torch.cuda.is_available():
        device_map = "auto"
        device_name = "GPU"
    else:
        print("âŒ é”™è¯¯: CUDAä¸å¯ç”¨ï¼Œè¯·ä½¿ç”¨ --device cpu")
        exit(1)
else:
    device_map = "cpu"
    device_name = "CPU"

# ç¡®å®šæ•°æ®ç±»å‹
dtype_map = {
    "float16": torch.float16,
    "bfloat16": torch.bfloat16,
    "float32": torch.float32,
}
model_dtype = dtype_map[args.dtype]

print("="*80)
print("llama-7B æ¨¡å‹æ ¼å¼è½¬æ¢å·¥å…·")
print("="*80)
print(f"æºæ¨¡å‹è·¯å¾„: {SOURCE_MODEL}")
print(f"ç›®æ ‡è·¯å¾„: {TARGET_MODEL}")
print(f"ä½¿ç”¨è®¾å¤‡: {device_name}")
print(f"æ¨¡å‹ç²¾åº¦: {args.dtype}")
if torch.cuda.is_available():
    print(f"GPUä¿¡æ¯: {torch.cuda.get_device_name(0)}")
    print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
print("="*80)
print()

# æ£€æŸ¥æºæ¨¡å‹æ˜¯å¦å­˜åœ¨
if not os.path.exists(SOURCE_MODEL):
    print(f"âŒ é”™è¯¯: æºæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {SOURCE_MODEL}")
    exit(1)

# åˆ›å»ºç›®æ ‡ç›®å½•
os.makedirs(TARGET_MODEL, exist_ok=True)
print(f"âœ… åˆ›å»ºç›®æ ‡ç›®å½•: {TARGET_MODEL}")

# æ­¥éª¤1 & 2: å¤åˆ¶tokenizerå’Œconfigæ–‡ä»¶
print("\næ­¥éª¤1-2: å¤åˆ¶tokenizerå’Œé…ç½®æ–‡ä»¶...")
try:
    files_to_copy = [
        "config.json",
        "generation_config.json", 
        "tokenizer_config.json",
        "special_tokens_map.json",
        "tokenizer.model",
        "tokenizer.json",
    ]
    
    for file_name in files_to_copy:
        src_file = os.path.join(SOURCE_MODEL, file_name)
        dst_file = os.path.join(TARGET_MODEL, file_name)
        if os.path.exists(src_file):
            shutil.copy2(src_file, dst_file)
            print(f"  âœ… {file_name}")
        else:
            print(f"  âš ï¸  {file_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
    
    print("âœ… æ–‡ä»¶å¤åˆ¶å®Œæˆ")
except Exception as e:
    print(f"âŒ æ–‡ä»¶å¤åˆ¶å¤±è´¥: {e}")
    exit(1)

# æ­¥éª¤3: è½¬æ¢æ¨¡å‹æƒé‡
print("\næ­¥éª¤3: è½¬æ¢æ¨¡å‹æƒé‡ (.bin -> safetensors)...")
if device_map == "auto":
    print("âš ï¸  æ³¨æ„: ä½¿ç”¨GPUåŠ è½½ï¼Œéœ€è¦è¶³å¤Ÿçš„GPUå†…å­˜ï¼ˆçº¦14GB+ï¼‰")
else:
    print("âš ï¸  æ³¨æ„: ä½¿ç”¨CPUåŠ è½½ï¼Œéœ€è¦è¶³å¤Ÿçš„RAMï¼ˆçº¦30GB+ï¼‰")
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")

try:
    # è®¾ç½®ç¯å¢ƒå˜é‡è·³è¿‡torchç‰ˆæœ¬æ£€æŸ¥ï¼ˆä»…ç”¨äºè½¬æ¢ï¼‰
    os.environ["TRANSFORMERS_NO_TORCH_LOAD_SAFEGUARDS"] = "1"
    
    print(f"ä½¿ç”¨{device_name}åŠ è½½æ¨¡å‹ï¼ˆç²¾åº¦: {args.dtype}ï¼‰...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰accelerate
    try:
        import accelerate
        has_accelerate = True
    except ImportError:
        has_accelerate = False
        if device_map == "auto" or device_map.startswith("cuda"):
            print("âš ï¸  accelerateæœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•çš„GPUåŠ è½½")
    
    if has_accelerate:
        model = AutoModelForCausalLM.from_pretrained(
            SOURCE_MODEL,
            torch_dtype=model_dtype,
            device_map=device_map,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
        )
    else:
        # ä¸ä½¿ç”¨device_mapï¼Œç›´æ¥æŒ‡å®šdevice
        print("åŠ è½½åˆ°CPUï¼Œç„¶åç§»åŠ¨åˆ°GPU...")
        model = AutoModelForCausalLM.from_pretrained(
            SOURCE_MODEL,
            torch_dtype=model_dtype,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
        )
        if device_map != "cpu":
            print("ç§»åŠ¨æ¨¡å‹åˆ°GPU...")
            model = model.to("cuda")
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # å¦‚æœä½¿ç”¨GPUï¼Œæ‰“å°æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available() and device_map != "cpu":
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            reserved = torch.cuda.memory_reserved(i) / 1024**3
            print(f"  GPU {i}: å·²åˆ†é… {allocated:.2f} GB, å·²ä¿ç•™ {reserved:.2f} GB")
    
    # ä¿å­˜ä¸ºsafetensorsæ ¼å¼
    print("æ­£åœ¨ä¿å­˜ä¸ºsafetensorsæ ¼å¼...")
    model.save_pretrained(
        TARGET_MODEL,
        safe_serialization=True,  # ä½¿ç”¨safetensorsæ ¼å¼
        max_shard_size="5GB",  # æ¯ä¸ªåˆ†ç‰‡æœ€å¤§5GB
    )
    print("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")
    
    # æ¸…ç†ç¯å¢ƒå˜é‡
    if "TRANSFORMERS_NO_TORCH_LOAD_SAFEGUARDS" in os.environ:
        del os.environ["TRANSFORMERS_NO_TORCH_LOAD_SAFEGUARDS"]
    
except Exception as e:
    print(f"âŒ æ¨¡å‹è½¬æ¢å¤±è´¥: {e}")
    print("\nå¯èƒ½çš„åŸå› :")
    if device_map == "auto":
        print("1. GPUå†…å­˜ä¸è¶³ - llama-7Béœ€è¦çº¦14GB+ GPUå†…å­˜")
        print("2. å°è¯•ä½¿ç”¨CPU: python convert_llama7b_to_safetensors.py --device cpu")
    else:
        print("1. CPUå†…å­˜ä¸è¶³ - éœ€è¦è‡³å°‘30GB+ RAM")
        print("2. å¦‚æœæœ‰GPUï¼Œå°è¯•ä½¿ç”¨GPUä¼šæ›´å¿«: python convert_llama7b_to_safetensors.py --device cuda")
    print("3. torchç‰ˆæœ¬é—®é¢˜ - å½“å‰ç‰ˆæœ¬æ— æ³•åŠ è½½.binæ–‡ä»¶")
    print("\nå»ºè®®:")
    print("1. ä½¿ç”¨float16æˆ–bfloat16å‡å°‘å†…å­˜ä½¿ç”¨")
    print("2. å…³é—­å…¶ä»–å ç”¨å†…å­˜çš„ç¨‹åº")
    print("3. æˆ–è€…ç›´æ¥ä»HuggingFaceä¸‹è½½safetensorsç‰ˆæœ¬çš„llama-7B")
    import traceback
    traceback.print_exc()
    exit(1)

# æ­¥éª¤4: éªŒè¯è½¬æ¢ç»“æœ
print("\næ­¥éª¤4: éªŒè¯è½¬æ¢ç»“æœ...")
safetensors_files = list(Path(TARGET_MODEL).glob("*.safetensors"))
if safetensors_files:
    print(f"âœ… æ‰¾åˆ° {len(safetensors_files)} ä¸ªsafetensorsæ–‡ä»¶:")
    for f in safetensors_files:
        size_mb = f.stat().st_size / (1024*1024)
        print(f"  - {f.name} ({size_mb:.2f} MB)")
else:
    print("âŒ é”™è¯¯: æœªæ‰¾åˆ°safetensorsæ–‡ä»¶")
    exit(1)

# æ­¥éª¤5: æµ‹è¯•åŠ è½½
print("\næ­¥éª¤5: æµ‹è¯•åŠ è½½è½¬æ¢åçš„æ¨¡å‹...")
try:
    # ä½¿ç”¨ç›¸åŒçš„è®¾å¤‡è¿›è¡Œæµ‹è¯•
    if has_accelerate:
        test_model = AutoModelForCausalLM.from_pretrained(
            TARGET_MODEL,
            torch_dtype=model_dtype,
            device_map=device_map,
            low_cpu_mem_usage=True,
        )
    else:
        test_model = AutoModelForCausalLM.from_pretrained(
            TARGET_MODEL,
            torch_dtype=model_dtype,
            low_cpu_mem_usage=True,
        )
        if device_map != "cpu":
            test_model = test_model.to("cuda")
    
    print("âœ… æ¨¡å‹åŠ è½½æµ‹è¯•æˆåŠŸ")
    
    # æ¸…ç†æµ‹è¯•æ¨¡å‹
    del test_model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
    exit(1)

print("\n" + "="*80)
print("ğŸ‰ è½¬æ¢å®Œæˆ!")
print("="*80)
print(f"åŸå§‹æ¨¡å‹: {SOURCE_MODEL}")
print(f"è½¬æ¢åæ¨¡å‹: {TARGET_MODEL}")
print("\nä¸‹ä¸€æ­¥:")
print("1. æ›´æ–°MODEL_CONFIGSä½¿ç”¨æ–°è·¯å¾„:")
print(f'   "llama-7B": "{TARGET_MODEL}"')
print("\n2. å–æ¶ˆæ³¨é‡Šllama-7Bé…ç½®:")
print("   åœ¨test_matching_eps_kv.pyå’Œvocab_matching_attack_kv.pyä¸­")
print("\n3. è¿è¡Œæµ‹è¯•:")
print("   ./run_kv_test.sh")
print("="*80)

